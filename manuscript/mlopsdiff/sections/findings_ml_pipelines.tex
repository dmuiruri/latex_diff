%\setcounter{subsection}{0}
%\renewcommand*{\theHsection}{chX.\the\value{section}}
\DIFaddbegin \DIFadd{This section presents common tools observed across the cases as summarized in Table~\ref{tab:databases}.
}\DIFaddend 



\DIFdelbegin %DIFDELCMD < \input{tables/tools_summary} %%%
%DIF <  Tooling summary table
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < The practices that are used in the operationalization of ML-enabled solutions are supported by a set of tools that cover model training and serving workflows. The training workflow is used to build models often in an offline mode. The trained models are run in production, often in an online mode, through the serving workflow. In this section, we present common tool chains of ML training and serving workflows useful for ensuring repeatable and traceable process of operationalizing ML solutions in industrial settings. 
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < \subsection{Provisioning execution infrastructure}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \subsection{Version management}
\DIFdelbegin \DIFdel{ML }\DIFdelend \DIFaddbegin \DIFadd{Model }\DIFaddend training code, often written in notebooks, \DIFdelbegin \DIFdel{as well as }\DIFdelend \DIFaddbegin \DIFadd{and }\DIFaddend other project artifacts are version controlled using tools like Git, Gitlab\DIFaddbegin \DIFadd{, }\DIFaddend and Bitbucket. Data versioning is done by generating and versioning metadata \DIFdelbegin \DIFdel{or }\DIFdelend using specialized tools\DIFaddbegin \DIFadd{, }\DIFaddend such as DVC. 
%DIF < Other project artifacts such files or scripts used for documenting: dependencies and versions of open-source libraries (requirements.txt), workflow steps (configuration scripts).
\DIFdelbegin \DIFdel{For many cases, training of ML models generally happens in a public cloud or on-premise servers. To quickly and consistently provision the execution environment for the training workflows}\DIFdelend \DIFaddbegin \DIFadd{Model training is conducted in public cloud settings for most cases, while a few cases train on-premises. To consistently provision training environments}\DIFaddend , 'infrastructure-as-code' practices, using tools like Terraform (\DIFdelbegin \DIFdel{https://bit.ly/2WIt724), are adopted in }\DIFdelend Cases A and E\DIFdelbegin \DIFdel{. Serving of inferences based on the trained models }\DIFdelend \DIFaddbegin \DIFadd{). Inference serving }\DIFaddend is either done in \DIFdelbegin \DIFdel{a batch format or online }\DIFdelend \DIFaddbegin \DIFadd{batch or online format}\DIFaddend .


\subsection{ML training workflow}
We observe that \DIFdelbegin \DIFdel{cases either containerize }\DIFdelend \DIFaddbegin \DIFadd{most case organizations containerize (using docker) }\DIFaddend individual workflow steps \DIFdelbegin \DIFdel{or encapsulate }\DIFdelend \DIFaddbegin \DIFadd{instead of encapsulating }\DIFaddend all workflow steps \DIFdelbegin \DIFdel{to run }\DIFdelend in a single container\DIFdelbegin \DIFdel{where the former is the preferred setup. Docker is the main applied container technology. Containerization is appealing because (1) it allows decoupling from the execution environment, (2) }\DIFdelend \DIFaddbegin \DIFadd{. In ML, containerization facilitates the isolation of }\DIFaddend different workflow tasks/steps\DIFdelbegin \DIFdel{can be isolated and (3) it makes the workflow traceable }\DIFdelend \DIFaddbegin \DIFadd{, making the workflow modular, traceable, }\DIFaddend and reproducible. \DIFaddbegin \DIFadd{We further note that containers are commonly orchestrated using Kubernetes, allowing easier migration of pipelines (or parts of it) across infrastructure vendors. }\DIFaddend Data transfers across workflow steps during training \DIFdelbegin \DIFdel{is }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend done using standard persistent volumes. However, large datasets may require using network mounts (Case F).
\DIFdelbegin \DIFdel{We further note that containers are commonly orchestrated using Kubernetes which allows model training to be executed in any environment that supports Kubernetes whether on-premise or in a public cloud environment. %DIF < , like Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) and Azure Kubernetes Service (AKS).
}\DIFdelend 

\DIFdelbegin \DIFdel{Model building steps are managed using a configuration tool (e.g YAML based) or a workflow toolkit to depict various workflow tasks/steps. A workflow }\DIFdelend \DIFaddbegin \DIFadd{ML workflows }\DIFaddend may include steps specifying feature extraction, model training\DIFaddbegin \DIFadd{, }\DIFaddend and validation. The complexity involved in these steps can vary depending on the ML domain. \DIFdelbegin \DIFdel{For complex ML models, low-level ML training workflow }\DIFdelend \DIFaddbegin \DIFadd{Workflows can be managed using a custom configuration tool (e.g., YAML-based) or a dedicated workflow toolkit. In complex ML setups, }\DIFaddend frameworks such as Argo (Case D) and Metaflow (Case G) are preferred\DIFdelbegin \DIFdel{mainly because of the tool's flexibility}\DIFdelend . We note that although high-level ML workflow platforms\DIFaddbegin \DIFadd{, }\DIFaddend such as AWS SageMaker\DIFdelbegin \DIFdel{are available in some organizations, such platforms were not preferred }\DIFdelend \DIFaddbegin \DIFadd{, provide an end-to-end integration advantage, they are also challenging to use }\DIFaddend when developing complex models \DIFaddbegin \DIFadd{due to inflexibility }\DIFaddend (Cases B, G).

%were considered while in use in some projects within the organisations (Cases B and G), were not used in the studied use cases due to complex requirements of the models, high model training costs and cloud provider dependency. % Note that Case G issues were related to no support for streaming interface and also cloud agnostic. Case A tooling requires adding a YAML file that contains workflow steps in the project repository. The steps vary depending on model complexity and can be visualized using a directional acyclic graph (DAG), which models each step and dependencies between them (Case F).

\DIFdelbegin \DIFdel{Proponents of dedicated ML training workflow tools prefer the end-to-end integration provided by such tools while those }\DIFdelend \DIFaddbegin \DIFadd{Those }\DIFaddend in support of custom tooling \DIFdelbegin \DIFdel{prefer the ability }\DIFdelend \DIFaddbegin \DIFadd{appreciate the flexibility }\DIFaddend to add different tools to the workflow. \DIFdelbegin \DIFdel{Typically, when a single task contains multiple containers, custom solutions involve implementing components or agents that provide an interface to a container for accessing data and compute resources during training (Cases C, G). In addition, components such as explainer dashboard (https://bit.ly/3Byj1js) (Case A) used to facilitate }\DIFdelend \DIFaddbegin \DIFadd{For example, }\DIFaddend a \DIFaddbegin \DIFadd{tool such as an explainer dashboard that facilitates a }\DIFaddend model's explainability \DIFdelbegin \DIFdel{can be }\DIFdelend \DIFaddbegin \DIFadd{is }\DIFaddend added as part of \DIFdelbegin \DIFdel{the workflow .
}\DIFdelend \DIFaddbegin \DIFadd{an ML workflow (Case A). An alternative workflow setting can have a single step containing multiple containers (data access data and model training). Customized components that provide access to these containers can be created to monitor independent utilization of computing resources at the container level (Cases C, G).
}\DIFaddend 

%DIF < dedicated workflow frameworks are preferred as opposed to custom solutions. When a single task contains multiple containers, custom solutions involves implementing components or agents that provide an interface to a container for accessing data and compute resources during training (Cases C, G). In addition, other components in the custom ML workflow solution, such as explainer dashboard (https://bit.ly/3Byj1js) (Case A) to facilitate a model's explainability if required.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < and pipeline orchestrator. The pipeline orchestrator of Case C ensures the transition of tasks between queues since each container has its queue
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF < The ML training workflow frameworks are used in some cases rather than a custom solution to provide automated end-to-end integration of workflow steps. . 
%DIF <  solutions 
 %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{After adding model trainingworkflow tools, data scientists can orchestrate event based training queues e.g., based on }\DIFdelend \DIFaddbegin \DIFadd{One overall advantage of using ML workflow tools is that event-based training queues can be orchestrated, for example, based on the }\DIFaddend continuous arrival of training data.  Tools like Apache airflow provide the functionality to schedule model training based on \DIFdelbegin \DIFdel{certain triggers.
%DIF < In Case 8, a consumer API is used to provide a common interface for accessing containers, actual data and the different queues.  
 }\DIFdelend \DIFaddbegin \DIFadd{given triggers.
}\DIFaddend 

\DIFdelbegin \DIFdel{To track model experiments , custom tools such as }\DIFdelend \DIFaddbegin \DIFadd{ML experiments can be tracked using custom }\DIFaddend web-based UI tools\DIFdelbegin \DIFdel{in Cases B and F are developed to facilitate }\DIFdelend \DIFaddbegin \DIFadd{; this facilitates the }\DIFaddend evaluation of results and \DIFdelbegin \DIFdel{compare model predictions with ground truth}\DIFdelend \DIFaddbegin \DIFadd{model performance comparison during the development process (Cases B and F)}\DIFaddend . To their advantage, custom platforms can \DIFdelbegin \DIFdel{include any data the team deems important }\DIFdelend \DIFaddbegin \DIFadd{freely include any metadata the team considers relevant }\DIFaddend (Case F). \DIFdelbegin \DIFdel{In other cases, plugins can }\DIFdelend \DIFaddbegin \DIFadd{Plugins can also }\DIFaddend be developed to integrate with existing open-source solutions \DIFdelbegin \DIFdel{like }\DIFdelend \DIFaddbegin \DIFadd{such as }\DIFaddend MLflow (Case G). Low-level training metrics are observed with Tensorboard (Case A).

\DIFdelbegin \subsection{\DIFdel{Continuous integration and testing}}
%DIFAUXCMD
\addtocounter{subsection}{-1}%DIFAUXCMD
\DIFdelend \DIFaddbegin \input{tables/tools_summary} %DIF >  Tooling summary table 
\DIFaddend 


\DIFdelbegin \DIFdel{Continuous integration or build }\DIFdelend \DIFaddbegin \subsection{\DIFadd{Continuous integration (CI) and testing}}

\DIFadd{CI }\DIFaddend tools, such as Jenkins, are used to run tests and build docker images based on model artifacts \DIFaddbegin \DIFadd{resulting }\DIFaddend from the training workflow (Cases A, D, G). Static code analysis and other tests \DIFdelbegin \DIFdel{to check whether a container works are performed when building the container images }\DIFdelend \DIFaddbegin \DIFadd{check general container functionality during the image building process }\DIFaddend (Case A, G). \DIFdelbegin \DIFdel{In addition, other domain-specific tests are executed to make certain that the }\DIFdelend \DIFaddbegin \DIFadd{Domain-specific tests are also executed to ensure the scope of a model's }\DIFaddend inputs and outputs \DIFdelbegin \DIFdel{of the model are still correct, thus extending the tests to the whole pipeline by performing tests on }\DIFdelend \DIFaddbegin \DIFadd{is unchanged. These tests generally extend testing to the entire pipeline using }\DIFaddend small amounts of input data (Case D, F, G). 
\DIFaddbegin 

\DIFaddend Docker images created from the CI system are (automatically) deployed to \DIFdelbegin \DIFdel{another (test/staging ) }\DIFdelend \DIFaddbegin \DIFadd{a staging }\DIFaddend environment for additional tests \DIFdelbegin \DIFdel{prior to deployment . %DIF < The docker images are of various sizes, for example up to 12GB in Case D. 
For Case C, the latter environment contains a copy of production data which due to restrictions was not accessible in other environments. Prior to deployment, the tests are performed to verify the type of data that the model APIaccepts }\DIFdelend \DIFaddbegin \DIFadd{before deployment to production. Typically, this may include testing the model API's data type }\DIFaddend (Case A, D), \DIFdelbegin \DIFdel{the models make }\DIFdelend \DIFaddbegin \DIFadd{ensuring a model makes sound }\DIFaddend predictions (Case E) and \DIFdelbegin \DIFdel{ensure }\DIFdelend \DIFaddbegin \DIFadd{also ensuring }\DIFaddend that the deployment procedure loads a serialized model into the relevant API \DIFaddbegin \DIFadd{endpoints}\DIFaddend .

Trained models are \DIFdelbegin \DIFdel{stored in different ways, including general data storage and }\DIFdelend \DIFaddbegin \DIFadd{generally stored in classic data storage solutions or dedicated }\DIFaddend container image registries. For \DIFdelbegin \DIFdel{the trained models}\DIFdelend \DIFaddbegin \DIFadd{example}\DIFaddend , Case E stores \DIFdelbegin \DIFdel{the }\DIFdelend \DIFaddbegin \DIFadd{a }\DIFaddend model and metadata about the model \DIFdelbegin \DIFdel{, including name and version number in }\DIFdelend \DIFaddbegin \DIFadd{to }\DIFaddend a PostgreSQL data warehouse. \DIFdelbegin \DIFdel{Built container images of trained models from the build system or test environment are uploaded to Nexus in Case C, and to the docker registry in Case G prior to production deployment }\DIFdelend \DIFaddbegin \DIFadd{Case C stores trained models in Nexus while case G uses docker registry to store the models before deployment to production}\DIFaddend .



%DIF <  Summary table for practices and challenges
\DIFdelbegin %DIFDELCMD < \input{tables/practices_challenges}
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \subsection{ML deployment and serving}
\DIFdelbegin \DIFdel{In many cases, trained }\DIFdelend \DIFaddbegin \DIFadd{The majority of the }\DIFaddend models are deployed as REST (Representational State Transfer) \DIFdelbegin \DIFdel{APIs }\DIFdelend \DIFaddbegin \DIFadd{API endpoints }\DIFaddend on public cloud or \DIFdelbegin \DIFdel{on-premise }\DIFdelend \DIFaddbegin \DIFadd{on-premises }\DIFaddend servers. Other deployment targets include embedding the model on the actual application\DIFaddbegin \DIFadd{, }\DIFaddend such as a mobile application (Case P) or deploying to IoT devices through over-the-air deployments \DIFaddbegin \DIFadd{or onsite installations}\DIFaddend .

\DIFdelbegin \DIFdel{For applications }\DIFdelend \DIFaddbegin \DIFadd{Models }\DIFaddend with strict inference latency requirements \DIFdelbegin \DIFdel{, a }\DIFdelend \DIFaddbegin \DIFadd{are deployed as }\DIFaddend gRPC (Google Remote Procedure Call) \DIFdelbegin \DIFdel{based API is used in Case C because it supports streaminginterfaces.
For ML solutions that need to serve inference in (or near) real-time, different strategies are continuously evaluated both at models level and services level (Case E, G). 
}\DIFdelend \DIFaddbegin \DIFadd{API endpoints. Case C business application has strict latency requirements and uses the gRPC, which supports streaming.
}\DIFaddend 

Most cases implement custom serving infrastructure\DIFaddbegin \DIFadd{, }\DIFaddend although emerging model serving systems like KFServing \DIFdelbegin \DIFdel{(https://bit.ly/3DBF1eZ) and Seldon (https://bit.ly/2Ybwuix) are being }\DIFdelend \DIFaddbegin \DIFadd{and Seldon are }\DIFaddend tested in Cases C and O\DIFaddbegin \DIFadd{, }\DIFaddend respectively. 

Finally\DIFaddbegin \DIFadd{, }\DIFaddend we note that \DIFdelbegin \DIFdel{deployment is often not performed by data scientists but }\DIFdelend \DIFaddbegin \DIFadd{data scientists often do not undertake deployment-related tasks, but these are done }\DIFaddend by other dedicated \DIFdelbegin \DIFdel{team members or platform teams }\DIFdelend \DIFaddbegin \DIFadd{teams with specialist knowledge, such as Kubernetes configuration }\DIFaddend (Case G)\DIFdelbegin \DIFdel{because it requires considerably low-level knowledge of Kubernetes}\DIFdelend .

%DIF < While data scientists build up until model container images are created for deployment, the actual process of deploying the containers to production is more involving requiring expertise and deep understanding of the tools used in the serving infrastructure e.g., Kubernetes, amongst other things. The platform teams are looking to automate the deployment process (Case G).
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Case D, models are deployed on inference servers, with separate clusters for client with high-traffic volume. A main server cluster runs about 20 different models at the same time and each model is several gigabytes of memory. The inference infrastructure is setup in a way that " but different versions of the server have different models, and the routing is automatically happening."?? Servers are made to automatically scale with increase in traffic when new models are deployed. Monitoring is done with Prometheus and grafana for dashboards. Other than ensuring tha the model is running, monitoring of model accuracy in production is not feasible especially when data is owned externally by the client (Case D). Due to strict separation of the data, the inference system serves the prediction instantly and no data is stored.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Case G ...And then so now, so the data scientists can like themselves go all the way to our supposedly working container. But then when you want to deploy that container, so actually live endpoint to do inference on them. At this point, we, you need some Kubernetes knowledge. I mean, in principle, the data scientists can do that themselves, but it is a more involved process. So usually, someone from the platform team helps them at this point. But this is an area of active investigation, how to make it more self serve. So that data scientists could go in like this sort of DevOps fashion, all the way from development to production.
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \subsection{Monitoring}
After model deployment, monitoring is performed at different levels of granularity. \DIFdelbegin \DIFdel{Most common monitoring is done for infrastructure where logging, monitoring}\DIFdelend \DIFaddbegin \DIFadd{The most common form of monitoring is undertaken for infrastructure management. Logging, monitoring, }\DIFaddend and alerting services and tools\DIFaddbegin \DIFadd{, }\DIFaddend like Prometheuse and Kubernetes logging (stackdriver)\DIFaddbegin \DIFadd{, }\DIFaddend are used to collect a cluster's performance metrics\DIFdelbegin \DIFdel{which are visualised }\DIFdelend \DIFaddbegin \DIFadd{. Metrics are then visualized }\DIFaddend on dashboards using tools like Grafana, Tableau\DIFdelbegin \DIFdel{or any }\DIFdelend \DIFaddbegin \DIFadd{, or }\DIFaddend other business analytics tools. For models deployed as API, model logging (e.g., model predictions) integration with services like Elasticsearch and BigQuery can be used to perform model health and quality checks in production\DIFaddbegin \DIFadd{, }\DIFaddend e.g., average accuracy on sampled log (Cases A, F\DIFaddbegin \DIFadd{, }\DIFaddend and O).
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  case G  would say that monitoring is something we are very well aware that is sorely needed. But that which is still like an open question, how to solve it pretty much are you like, including what sort of operational teams to to be involved kind of maintaining the system ..And, yeah kind of we haven't yet gone into like the SLA s and stuff like that, because it's the lot in production. So we have been able to postpone it. But there are different while there's monitoring, and so many different levels, so, for example, you would want to know, like even when you're training or, or like you, if there's something going wrong, you need to be able to pinpoint at which point on which machine, something went wrong, and probably you won't be able to replicate the whole training process with the same data set this whole replication issue. And I feel like, we are probably not very, in a very good position with that, because the data platforms are not that we are using are not like we dislike data. versioning stuff, I think, is something we will probably run into later. But it is not solved yet.So this true monitoring on the training level, we have some tracing stuff. So we like on the network level, we know how the requests go. But that is like a super low level stuff. And if you there, we don't have like this, we would want to have is like the single pane of glass where we could see that this model is not being trained and it's moving, MLflow tries to be that a little bit, but it doesn't give like an active view into many parts of the system.And then on the endpoint level, of course, like when you have actually endpoints running and I mean, we will need endpoint level monitoring, where there are health checks and all kinds of quality metrics flowing in all the time, but that's not in place.
%DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <  Summarise RQ2, the pipeline section
\DIFdelend Maintaining the collective set tools used across a pipeline can develop into a complex task\DIFaddbegin \DIFadd{, }\DIFaddend especially when dealing with NN architectures.


\DIFdelbegin \DIFdel{Generally, we observe that practices vary across organizations based on factors such as the type of data being used for training, availability of computing resources and type of ML solution being developed. We also note there are two primary ways case organizations develop their pipelines, (1) compose a variety of tools to orchestrate the pipeline or (2) apply a more encompassing framework such as Sagemaker which contains inbuilt tools for different parts of a pipeline. We note that most teams prefer flexibility and the ability to extract low level information provided by independent tools, i.e. the first approach. While the few teams that use the second approach prefer the instant integration of tools provided by their cloud provider. We summarize all general practices in Table~\ref{tab:practices_challenges}.
 }\DIFdelend %DIF >  case G  would say that monitoring is something we are very well aware that is sorely needed. But that which is still like an open question, how to solve it pretty much are you like, including what sort of operational teams to to be involved kind of maintaining the system ..And, yeah kind of we haven't yet gone into like the SLA s and stuff like that, because it's the lot in production. So we have been able to postpone it. But there are different while there's monitoring, and so many different levels, so, for example, you would want to know, like even when you're training or, or like you, if there's something going wrong, you need to be able to pinpoint at which point on which machine, something went wrong, and probably you won't be able to replicate the whole training process with the same data set this whole replication issue. And I feel like, we are probably not very, in a very good position with that, because the data platforms are not that we are using are not like we dislike data. versioning stuff, I think, is something we will probably run into later. But it is not solved yet.So this true monitoring on the training level, we have some tracing stuff. So we like on the network level, we know how the requests go. But that is like a super low level stuff. And if you there, we don't have like this, we would want to have is like the single pane of glass where we could see that this model is not being trained and it's moving, MLflow tries to be that a little bit, but it doesn't give like an active view into many parts of the system.And then on the endpoint level, of course, like when you have actually endpoints running and I mean, we will need endpoint level monitoring, where there are health checks and all kinds of quality metrics flowing in all the time, but that's not in place.
\DIFaddbegin 

%DIF >  Summarise RQ2, the pipeline section

 \DIFaddend