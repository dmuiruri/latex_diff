% Monitoring for Model quality using accuracy metrics, monitoring data drift using things like histograms, descriptive statistics .  
\subsection{Model monitoring} 
%DIF < ensure that a model does not regress in its utility, and, if it does regress, the reason for the regression should be easy to identify. This necessitates monitoring activities as part of an ML development workflow which mainly takes place at three levels -- data, model, and infrastructure. 

\underline{\DIFdelbegin \emph{\DIFdel{Training data drift.}}%DIFAUXCMD
\DIFdelend \DIFaddbegin \emph{\DIFadd{Training data drift}}\DIFaddend } \DIFdelbegin \DIFdel{Data level monitoring is instrumented to check for drift within the data }\DIFdelend \DIFaddbegin \DIFadd{commonly occurs due to structural changes in the data generating process}\DIFaddend . Identifying drift in numeric data types is be achieved by using visual tools such as graphs or descriptive statistics (cases G, H, I), image-based data makes use of histograms (case F). Speech and \DIFdelbegin \DIFdel{text based }\DIFdelend \DIFaddbegin \DIFadd{text-based }\DIFaddend data is also susceptible to drift but can be more challenging to monitor. For example, case D mentioned the emergence of the word Covid-19 in the medical sphere recently\DIFaddbegin \DIFadd{, }\DIFaddend but the word is not available in any historical corpus. Typically, heuristics are used to monitor drift in these speech or NLP settings.

\underline{\emph{Model drift}} \DIFdelbegin \DIFdel{Model monitoring involves ensuring a }\DIFdelend \DIFaddbegin \DIFadd{can occur as a result of data or concept drift and it is often manifested by a loss in a }\DIFaddend model's accuracy\DIFdelbegin \DIFdel{and error metrics are maintained at a certain threshold. Observing key }\DIFdelend \DIFaddbegin \DIFadd{. Observing }\DIFaddend metrics such as accuracy and error rates \DIFdelbegin \DIFdel{were observed as }\DIFdelend \DIFaddbegin \DIFadd{are }\DIFaddend the common ways production models are monitored. For example, in a transcription setting, measuring the character and word edits required after inference were used (Case D) as error metrics \DIFdelbegin \DIFdel{used to monitor already deployed }\DIFdelend \DIFaddbegin \DIFadd{to monitor production }\DIFaddend models and to characterise any drift in the model.

\underline{\DIFdelbegin \emph{\DIFdel{Infrastructure utilization}}%DIFAUXCMD
%DIFDELCMD < \MBLOCKRIGHTBRACE %%%
\DIFdel{Infrastructure monitoring is commonly }\DIFdelend \DIFaddbegin \emph{\DIFadd{Infrastructure monitoring}}}
\DIFadd{is }\DIFaddend applied to ensure \DIFaddbegin \DIFadd{models utilize }\DIFaddend resources (GPU/CPU, memory, disk, network, etc.) \DIFdelbegin \DIFdel{are utilized reasonably }\DIFdelend \DIFaddbegin \DIFadd{efficiently }\DIFaddend or to flag \DIFdelbegin \DIFdel{unnoticed }\DIFdelend technical problems such as scaling designs \DIFdelbegin \DIFdel{, IO bottlenecks and how inferenceendpoints utilize backend resources. Inference endpoints often require further low-level monitoring to characterize the latency of the solution. }\DIFdelend \DIFaddbegin \DIFadd{and I/O bottlenecks during training or inference. }\DIFaddend Cases D, E and G \DIFdelbegin \DIFdel{were particularly keen to }\DIFdelend \DIFaddbegin \DIFadd{closely }\DIFaddend monitor endpoint latency \DIFdelbegin \DIFdel{as this formed }\DIFdelend \DIFaddbegin \DIFadd{since it forms }\DIFaddend an important requirement of the \DIFaddbegin \DIFadd{entire }\DIFaddend ML solution.

%Case organizations are aware of the benefits of monitoring different parts  of the entire ML pipelines but often times end up monitoring only certain stages of the pipelines based on their perceived importance.

