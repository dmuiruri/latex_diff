% Summary context, data collection, storage platforms
\input{tables/data_source_storage_mlframeworks}


\subsection{Data management}
%DIF < Generally, data used to train the case ML systems is largely unstructured or semi-structured data, which in its basic form is composed of images/video, pdf files, text files, or genome sequencing data. In a few instances structured data is used to generate analytics with supporting ML based features. 

\underline{\emph{Data collection and storage}}
% Batch vs Real-time streams
\DIFdelbegin \DIFdel{typically begins either by }\DIFdelend \DIFaddbegin \DIFadd{are handled in various ways: }\DIFaddend batch loading data from internal systems, streaming from devices/sensors, extracting from \DIFdelbegin \DIFdel{other }\DIFdelend third-party vendors \DIFdelbegin \DIFdel{through APIs or from }\DIFdelend \DIFaddbegin \DIFadd{APIs or }\DIFaddend open-source repositories. The training data is then commonly stored in cloud platforms, as shown from data sources in Table ~\DIFdelbegin \DIFdel{\ref{tab:data_source_storage_mlframeworks}.
%DIF < or private infrastructure, a summary of the storage solutions  is presented in column "Storage Solution" of Table~\ref{tab:data_source_storage_mlframeworks}. Some organizations have multiple storage solutions, we only present the identified source of ML training data.
}\DIFdelend \DIFaddbegin \DIFadd{\ref{tab:data_source_storage_mlframeworks_interviewees}.
}\DIFaddend 

%DIF < Often this is due to data residency requirements where the data is required to reside in Finland. 
\DIFdelbegin \DIFdel{Organizations often use cloud storage providers with data centres either in Finland, close proximity to Finland or within the European Union geographic area following customer preferences or regulatory constraints. In very strict circumstances, data is stored in private infrastructure.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Additionally, low-level metrics , }\DIFdelend \DIFaddbegin \DIFadd{Low-level metrics }\DIFaddend such as IOPS (I/O Operations Per Second) \DIFdelbegin \DIFdel{, }\DIFdelend are considered when choosing a storage architecture\DIFdelbegin \DIFdel{as }\DIFdelend \DIFaddbegin \DIFadd{; }\DIFaddend data fetching can constitute a sizeable amount of the overall model training time. Case E uses \DIFaddbegin \DIFadd{a }\DIFaddend mounted discs solution instead of a network drive accessed via a web interface. %I/O bound process.

\underline{\emph{Data storage formats}} are \DIFdelbegin \DIFdel{also important architectural decisions when considering }\DIFdelend \DIFaddbegin \DIFadd{factored in when considering the }\DIFaddend scalability of data processing pipelines, \DIFdelbegin \DIFdel{portability of data }\DIFdelend \DIFaddbegin \DIFadd{data portability }\DIFaddend between computing environments, \DIFaddbegin \DIFadd{and }\DIFaddend support of different ML frameworks\DIFdelbegin \DIFdel{and programming languages. In this regard, two data storage formats are presented: the Apache Parquet (https://bit.ly/3kGFaVI) and the NetCDF (https://bit.ly/3zy287R) file formats.
}%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \DIFaddbegin \DIFadd{. }\DIFaddend Case H uses Apache Parquet in \DIFdelbegin \DIFdel{favour }\DIFdelend \DIFaddbegin \DIFadd{favor }\DIFaddend of CSV (Comma Separated Values) or TSV (Tab Separated Values) file formats commonly used to store structured data for analytics purposes. Case G uses NetCDF \DIFdelbegin \DIFdel{as a solution }\DIFdelend to implement a generic data interface to abstract data across ML frameworks and computing platforms. 
\DIFdelbegin \DIFdel{Data scientists then ensure models can process NetCDF input and produce NetCDF output.
}\DIFdelend 

% \subsubsection{Data versioning}

\underline{\emph{Data discoverability and accessibility}}
\DIFdelbegin \DIFdel{affects the rate of experimentation and development of ML features. Discoverability tends to be a concern }\DIFdelend %DIF > affects the rate of experimentation and development of ML features. This 
\DIFaddbegin \DIFadd{is emphasized }\DIFaddend in setups that feature a data lake or where different \DIFdelbegin \DIFdel{types of data }\DIFdelend \DIFaddbegin \DIFadd{data types }\DIFaddend are collected. Case O describes a solution to this problem based on maintaining a \textit{data \DIFdelbegin \DIFdel{catalogue}\DIFdelend \DIFaddbegin \DIFadd{catalog}\DIFaddend } where data and its value are described. %This supports faster identification of data and its associated value. An alternative approach is to assign metadata to ingested data and the metadata can be made visible through custom tools.
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{Data access is a concern whenever an organization handles personal data or requires collaboration with third parties e.g. in consultancy settings. The process to grant data access can be lengthy and can result in data governance anti-patterns}\DIFdelend \DIFaddbegin \DIFadd{Data governance and related processes can limit the use and scope of data accessible for ML purposes}\DIFaddend .
%An example of such a data migration pattern involves an organization streaming data from its IoT devices to it's own cloud storage then a subset of the data is sent to a third party entity processor for cleaning or model training over an agreed interval. %https://www.linkedin.com/pulse/top-10-data-governance-anti-patterns-analytics-dave-wentzel

\underline{\emph{Data validation}}%DIF < Data ETL pipelines often include practices and mechanisms that ensure continuous collection or storage of clean data. Topical issues discussed relate to technical approaches, organisational set ups applied to collect good quality data and maintain data integrity.
techniques are commonly applied as a means of controlling data quality. However, data types influence the type of validation approaches \DIFdelbegin \DIFdel{applied. }\DIFdelend \DIFaddbegin \DIFadd{used. Validation of }\DIFaddend Image/video, speech\DIFaddbegin \DIFadd{, }\DIFaddend and text tend to require human actors supported by custom tools\DIFdelbegin \DIFdel{to validate and ensure data meets aspired quality thresholds. E.g, in an object detection setting, }\DIFdelend \DIFaddbegin \DIFadd{. For example, }\DIFaddend a human validator ensures that objects fall within the annotated bounding boxes \DIFdelbegin \DIFdel{. With speech , validation ensures that }\DIFdelend \DIFaddbegin \DIFadd{in an object detection setting. While a human speech validator ensures }\DIFaddend recorded utterances are coherent and consistent with corresponding text. Case D uses additional heuristics for detecting anomalies between generated texts and submitted utterances. Numerical data types \DIFdelbegin \DIFdel{normally easier to automatically validate .
%DIF < Other images can be validated by generating small image thumbnails which allow for a quick preview of the data or by using distribution graphs. 
}\DIFdelend \DIFaddbegin \DIFadd{usually are easier to validate automatically.
}\DIFaddend 

Data validation in Case O is done at a schema and data level. \DIFdelbegin \DIFdel{The schema is maintained by dedicated }\textit{\DIFdel{data stewards}} %DIFAUXCMD
\DIFdel{team that ensures the schemareflects the required data}\DIFdelend \DIFaddbegin \DIFadd{A dedicated data stewards maintain the schema}\DIFaddend . Delegating quality control ensures a team managing the data lake ingests data indiscriminately. When data is sourced from third-party vendors, the vendor is expected to maintain quality controls (Case P).
%DIF <  Control alarms are directed to this team whenever there is a quality breach such as incoming data not conforming to the defined schema or having erroneous data values.


\underline{\emph{Data integrity}}
controls ensure data is not changed unexpectedly. Case D and F apply hashing as part of data processing pipelines\DIFaddbegin \DIFadd{; }\DIFaddend this ensures training data is verifiable and traceable with respect to a model's lineage. Additionally, this practice ensures that attempts to overwrite data are flagged appropriately.

Generally, when hashing is not a suitable approach\DIFdelbegin \DIFdel{for example }\DIFdelend \DIFaddbegin \DIFadd{, for example, }\DIFaddend when dealing with image files, other custom tooling and heuristics are used to perform anomaly detection\DIFdelbegin \DIFdel{, }\DIFdelend \DIFaddbegin \DIFadd{; }\DIFaddend Cases B and I make use of this approach.
%DIF <  Case B has a custom web tool based on anomaly detection algorithms used to manage data quality and improve training data. %explain the solution in I

%DIF <  Case B
%DIF <  
%DIF <  So I'm in charge, partly for the model and for those algorithms, but then also my duty is the anomaly detection section of our system, because we have challenges with the quality of the training data. So we need an additional anomaly detection system on top of it, I am in charge of that system. And then also, I'm in charge of the user interface, we have a web interface for monitoring this for visualizing the issues with our controls. And also that same UI will be used for monitoring responses, predictions of the model in production. So you will use the same UI for monitoring the results of the system. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend \underline{\emph{Data labelling and annotation}} \DIFdelbegin \DIFdel{tends }\DIFdelend \DIFaddbegin \DIFadd{tend }\DIFaddend to be undertaken manually using custom tools developed to standardize the process. %DIF < We noted that instrumenting labels or verification of labels was an ongoing challenge in most case organizations due to lack of standardised tools.
Inconsistent labels are \DIFdelbegin \DIFdel{time to time }\DIFdelend \DIFaddbegin \DIFadd{sometimes }\DIFaddend encountered due to subjective interpretations\DIFdelbegin \DIFdel{therefore }\DIFdelend \DIFaddbegin \DIFadd{, }\DIFaddend resulting in poor data quality. \DIFdelbegin \DIFdel{To overcome such issues }\DIFdelend Case B implements a standardized way of normalizing and giving common meaning to concepts \DIFdelbegin \DIFdel{. %DIF < e.g., use invoice date in place of terms like date received, processed date or issued date etc. 
}\DIFdelend \DIFaddbegin \DIFadd{to overcome such issues. 
}\DIFaddend 

\underline{\emph{Data understanding}} requires domain knowledge for teams to generate valuable insights from data in \DIFdelbegin \DIFdel{specialised }\DIFdelend \DIFaddbegin \DIFadd{specialized }\DIFaddend domains. Domain knowledge is cited as a necessity in the entire life cycle of the data. \DIFdelbegin \DIFdel{E.g when }\DIFdelend \DIFaddbegin \DIFadd{For example, }\DIFaddend handling data from chemical processes or mechanical parts of large systems \DIFaddbegin \DIFadd{is }\DIFaddend represented in cases I, J\DIFaddbegin \DIFadd{, }\DIFaddend and L.
%DIF < Larger organizations with specialised teams can incur additional overhead trying to understand the value of data.

In general, challenges in data management practices are mainly attributed to data quality aspects. \DIFdelbegin \DIFdel{E.g. }\DIFdelend \DIFaddbegin \DIFadd{For example, }\DIFaddend sensor problems, inconsistent \DIFdelbegin \DIFdel{labelling}\DIFdelend \DIFaddbegin \DIFadd{labeling}\DIFaddend , programming errors in data handling software\DIFdelbegin \DIFdel{etc.
%DIF <  Case B
%DIF <  our ground truth preparation column and  it's drawn in the image of oil refinery because we do this cleaning of our ground truth in multiple stages and the process is sequential and it resembles this kind of refinery, on each layer you get the little bit better something and this is how our preparation goes. To this pipe but you have two inputs, one is PDFs and another is ground truth data taken from the database data just tells that okay this invoice total value is 100 and then due date is first of May and then invoice number is ABC. So, this is textual data that just tells the final values of the fields of the invoice and then PDFs which are invoices, this is our training data. 
}\DIFdelend \DIFaddbegin \DIFadd{, etc.
}\DIFaddend 



 %DIF <  \DIFdelbegin \DIFdel{Consider replacing company column in table III with sector information but cases H and J might require non disclosure. }\DIFdelend