
%DIF > %%%%%%%%% Reviewer 2 %%%%%%
%DIF >  Most importantly, a more in-depth discussion on the results is expected. For example, what can we learn from the survey results, especially for the SE4AI or just the AI community? Does the result imply new challenges or directions in these communities? The discussion in Section 6 is superficial currently and is just informative to the practitioners who would like to build a ML production system.
\DIFaddbegin 

%DIF > %%%%%%%%% Reviewer 3 %%%%%%

%DIF >  3. The paper can benefit from a broader discussion about the practices distilled and how they compare to related work. For example, with reference [2], which served as the main inspiration, and reference [5]. Is the taxonomy used at least similar to [2]?

%DIF >  4. A similar discussion is needed for the distilled conclusions. For example, are the practices presented in the paper adopted in a similar manner as in, say, reference [5]? If not, is this because of regional bias or time differences?

\DIFaddend % Summarize and generalize the findings
\DIFdelbegin \DIFdel{In relation }\DIFdelend \DIFaddbegin \DIFadd{As the AI engineering field is still making progress in defining well-established processes, there is a need for details on how such systems are engineered~\mbox{%DIFAUXCMD
\cite{9121629}}\hskip0pt%DIFAUXCMD
. Compared }\DIFaddend to existing literature, our findings \DIFdelbegin \DIFdel{provide explanations to the low adoption rates of the important best practices in ML \mbox{%DIFAUXCMD
\cite{Serban2020Practices}}\hskip0pt%DIFAUXCMD
. In addition, compared to experienced online data-intensive organizations , our study show that majority of organizations have not achieved the capability required to enable continuous deployment and operations of ML models in production . However, there is great awareness of these amongst the practitioners as evidenced by their continued efforts to improve their practices and tools. We make the following observations concerning the current state of practice  in many organizations. These areas would greatly benefit from future research contributions.}\DIFdelend \DIFaddbegin \DIFadd{of practices in ML workflows (Section 4) and tools in ML pipelines (Section 5) presents the ‘how’ knowledge, in contrast to cataloging the practices or tools used in ML applications. This information can be used to objectively analyze practices applied across organizations and identify areas to guide future research that seeks to improve knowledge in the field of AI engineering.
}\DIFaddend 

%DIF < Highlight the most important or most surprising observations
\DIFdelbegin \DIFdel{First, streamlining data handling practices in order to get good quality }\DIFdelend \DIFaddbegin \DIFadd{Following the taxonomy and challenges described in~\mbox{%DIFAUXCMD
\cite{Lwakatare2019}}\hskip0pt%DIFAUXCMD
, we consider the presented cases to be at the critical deployment stage where ML components need to co-exist with other general software components in production systems. We observed that practitioners were implementing practices that also address all of the challenges associated with this critical deployment stage~\mbox{%DIFAUXCMD
\cite{Lwakatare2019}}\hskip0pt%DIFAUXCMD
. In particular, tools and practices adopted in ML workflows that are summarised in Table~\ref{tab:databases} and Table~\ref{tab:practices_challenges} respectively are indicative of these solutions. However, the challenge related to data management remains an active research area given the increasing amount of data and disparity in data types. Similarly, the challenges in subsequent stages also need to be addressed, particularly new techniques for monitoring the final models observed in both studies.
}

%DIF >  Practices distilled and how they compare to related work
\DIFadd{Similar to \mbox{%DIFAUXCMD
\cite{Serban2020Practices}}\hskip0pt%DIFAUXCMD
, our results reveal a low adoption of SE best practices in the deployment category and a medium-to-high adoption in other categories (data, training, team, and coding). We also observed that smaller and younger organizations found it easier to adopt SE practices and emerging tools than older organizations with larger and distributed teams. We attribute this disparity to legacy systems and processes in the older organizations.
}

\DIFadd{While the survey indicated that teams with low experience have low adoption of the SE practices, we observed the contrary that there was a high adoption of the SE practices in teams with limited experience. We attribute this as mainly due to the field of AI engineering having clearly defined the specific practices (e.g., tracking of model experiments) with supporting tools (MLflow, Kubeflow) in academia and industry. There remain several areas in the AI engineering field where certain practices, e.g., }\DIFaddend data \DIFaddbegin \DIFadd{versioning, are considered valid but lack well-established knowledge of how to enact the practice and thus are least adopted in the industry.
}


\subsection{\DIFadd{Implications to research}}
\textit{\DIFadd{Practices and tools for data discoverability.}} \DIFadd{Obtaining good quality training data for ML purposes }\DIFaddend is an arduous task \DIFdelbegin \DIFdel{given increasing amounts of data , challenging data types and widening regulatory oversight around personal data . As a result, data handling workflows tend to induce long ML production cycles}\DIFdelend \DIFaddbegin \DIFadd{more so due to the increasing amount of data being produced and the variability of data handling procedures across different data types}\DIFaddend . Establishing efficient data \DIFdelbegin \DIFdel{handling }\DIFdelend \DIFaddbegin \DIFadd{discoverability }\DIFaddend procedures would shorten ML production cycles and increase experimentation \DIFaddbegin \DIFadd{of ML models }\DIFaddend for R\&D purposes.

%DIF <  the amounts of data to use for ML is increasing but is often arriving at long time intervals,  especially for organisations that handle either sensitive personal data (public agency, healthcare, pharmaceutical) or data collection is done by external parties (consultancies, Energy). This shows that most organisations are not yet dealing with real-time data, which would further complicate the ML pipelines. However, the late arrival of data lengthens the release cycle time (and the rate of updates) of ML models in production. As such, organizations need separate and well-established access procedures and contracts for using sensitive data for R\&D. In addition, different techniques are needed to ensure quality in each data update, for example when older data gets updates from a power plant on next day or gets overwritten due to logging faults in data collection software.
\DIFdelbegin \DIFdel{Second, maintaining a ML system involves monitoring and evaluating procedures to ensure high availability and to control for model staleness respectively.Data distributions may change over time meaning that models may have an implicit viability lifespan. Due to the availability of tools, teams tend to focus on monitoring infrastructure while modelevaluation tends to be conducted on a case by case basis. Issues such as model explainability, control for bias, feature attribution continues to be an open challenges from a tooling perspective and developing practice. 
}\DIFdelend \DIFaddbegin \DIFadd{Although feature stores have emerged as an intermediate solution for managing data in ML settings, there lacks empirical research on their adoption rates, benefits, and applicability across various data types and business settings.%DIF >  Why are practitioners yet to adopt the use of feature stores? feature engineering
%DIF >  for towards solving data discoverability constraints
%DIF > Larger datasets are harder and costly to clean and maintain
%DIF > and widening regulatory oversight around personal data
}\DIFaddend 

%DIF <  Second, the data collection practices for ML in most of the organizations do not largely use or incorporate feedback from inference data. The organizations that are working on well-established ML problems, such as speech recognition, chatbots and impressions in marketing, tend to have data collection practices wherein inference data is logged and used to build training data corpus. For ML problems in IoT, the data collection practices in the organizations deal with large scale amounts of data from IoT devices as well as multiple data integration from heterogeneous sources. The latter requires establishing practices to effectively store as well as easily discover relevant data and process the complex domain features of the collected data. At the same time, even though the deployment of ML models on the edge devices can be done with tools (e.g., AWS IoT Greengrass and Azure IoT edge) there is a need to address security and safety concerns. Presently, monitoring practices at the edge are largely for robust connectivity and data flows.
\DIFaddbegin \textit{\DIFadd{Practices and tools for testing the ML models and monitoring them in production.}} \DIFadd{We observed a lack of extensive end-to-end testing of ML pipelines in the studied cases. In some cases though, static analyses were performed on the ML training code and the resulting container images were tested. 
}\DIFaddend 

%DIF <  Monitoring at different stages of ML pipepines
\DIFaddbegin \DIFadd{We further note that monitoring of ML-enabled system needs to extend beyond the general infrastructure monitoring practices. A model's utility can be reduced by degrading accuracy levels as a result of model drift. Although data drift can be detected during development, concept drift is more challenging to control in production settings. The empirical effects of concept drift and control practices are yet to be explored in literature.
}\DIFaddend 

\DIFdelbegin \DIFdel{Third, we observe that a significant number of organizations use a wide collection }\DIFdelend %DIF > Data distributions may change over time meaning that models may have an implicit viability lifespan. Due to the availability of tools, teams tend to focus on monitoring infrastructure while model evaluation tends to be conducted on a case by case basis. Issues such as model explainability, control for bias, or feature attribution continue remain open challenges from a tooling perspective and developing practice.
\DIFaddbegin 

\subsection{\DIFadd{Implication to practice}}
\textit{\DIFadd{Platforms vs .independent tools in ML.}}\DIFadd{Generally, we make a similar observation to \mbox{%DIFAUXCMD
\cite{Doris2021MLPipelines} }\hskip0pt%DIFAUXCMD
that practices in ML workflows and pipelines vary based on factors, such as the type of data being used in model training, availability of computing resources, and the type of ML solutions being developed. However, we also note two primary ways organizations developed their ML pipelines. One, they can compose a variety }\DIFaddend of tools to \DIFdelbegin \DIFdel{support their ML development as opposed to using an integrated MLOps platform. This is mainly due to desired flexibility and access to }\DIFdelend \DIFaddbegin \DIFadd{orchestrate a pipeline. Two, teams can use integrated frameworks/platforms such as SageMaker, which contain inbuilt tools for various stages of an ML pipeline. Most of the studied teams preferred the first approach because it offers flexibility and the ability to extract }\DIFaddend low-level \DIFdelbegin \DIFdel{features when necessary. Some topical areas with defacto tooling include version management , containerization and monitoring. Other areas continue to have multiple tooling options. We further note that there are industry wide efforts }\DIFdelend \DIFaddbegin \DIFadd{information provided by independent tools. However, a common challenge when using separate tools is the required high maintenance efforts. The few teams that use the second approach preferred the instant integration and support offered by platform providers.
}

\textit{\DIFadd{Dominant tools.}} \DIFadd{Some well-established tools in SE remain useful when engineering AI systems. Such tools relate to version management (Git), containerization (Docker), and monitoring (Prometheus). However, some of these tools are arguably insufficient for other AI artifacts. For example, code version management tools are not suitable for data version management. Alternative tools dedicated to ML settings are emerging to address some of the inefficiencies encountered by practitioners. %DIF >  Containers resulting from NN can be quite large to the 
}

%DIF >  Some well-established tools in SE remain useful when engineering AI systems. Such tools relate to version management (Git), containerization (Docker), and monitoring (Prometheus). However, some of these tools are arguably insufficient for other AI artifacts. For example, code version management tools are not suitable for data version management. Alternative tools dedicated to ML settings are emerging to address some of the inefficiencies encountered by practitioners.

%DIF > We observe that a significant number of organizations use a wide collection of tools to support their ML development as opposed to using an integrated MLOps platform. This is mainly due to desired flexibility and access to low-level features when necessary. 

\textit{\DIFadd{Model inference infrastructure.}} \DIFadd{Following general SE practices, ML API endpoints are based on custom webservers. However, we note there are initiatives }\DIFaddend to standardize model serving \DIFaddbegin \DIFadd{servers }\DIFaddend through open development of a serving API\DIFaddbegin \DIFadd{, }\DIFaddend which is realized by \DIFdelbegin \DIFdel{products such as }\DIFdelend \DIFaddbegin \DIFadd{frameworks and tools like }\DIFaddend NVIDIA's Triton Inference Server\DIFdelbegin \DIFdel{(https://bit.
ly/3Brm3Wv).
}\DIFdelend \DIFaddbegin \DIFadd{.
}\DIFaddend 


%DIF < Topical areas such as version management, containerization, container orchestration and monitoring have defacto solutions. Model serving is mainly REST based In addition, none of the organizations are using ML model serving tools, such as TensorFlow Serving, and model registry tools with up-to-date view of running ML pipelines and the versioned data sources in use. This observation is more pronounced in large organization that are in early stages of operationalizing ML-enabled software system. Lack of mature tools and the need for the tools to support specific technology stack hinder the adoption of the existing tools. For example, most organizations started by developing custom deployment servers to serve their ML solutions. Meanwhile, there are efforts in the industry to standardise the model serving process where ML frameworks provide prepackaged servers that are can be easily containerised for immediate deployment. Additionally, huge efforts and expertise are required to integrate and maintain the tools.
\DIFaddbegin \subsection{\DIFadd{Validity Threats}}
\DIFaddend 

%DIF < Third, with the common practice of using data from sources outside the control of the development team most organizations do not have in place well-established mechanisms of detecting any changes in data that could lead to system disruptions. Data validation tools proposed in literature to help with the continuous tracking of data quality are not used due to their limited capability but in some cases the task is delegated to the third party vendors. A practice complimentary to data-validation is to assess the performance of ML model when deciding whether a trained model can pushed to the serving stack. This is mostly practiced by the organization during evaluation of trained models and after integration tests wherein accuracy of ML model is evaluated on large set of unseen data. Generally, no observed controls for potential model bias drift or feature attribution/explainability.
\DIFaddbegin \textit{\DIFadd{Construct validity.}} \DIFadd{considers whether the constructs discussed in the interview questions were interpreted in the same way by the researchers and the interviewees. This was mitigated by sharing the study objective and an outline of the interview guide to practitioners before the interview. During the interviews, a brief presentation was given by researchers to communicate the interview framework, and later the discussion was tailored to practitioners' expertise.
}\DIFaddend 


%DIF < Fourth, a significant number of organizations employ custom tools rather than using MLOps platforms in training and tracking the different experiment of ML models. In addition, none of the organizations are using ML model serving tools, such as TensorFlow Serving, and model registry tools with up-to-date view of running ML pipelines and the versioned data sources in use. This observation is more pronounced in large organization that are in early stages of operationalizing ML-enabled software system. Lack of mature tools and the need for the tools to support specific technology stack hinder the adoption of the existing tools. For example, most organizations started by developing custom deployment servers to serve their ML solutions. Meanwhile, there are efforts in the industry to standardise the model serving process where ML frameworks provide prepackaged servers that are can be easily containerised for immediate deployment. Additionally, huge efforts and expertise are required to integrate and maintain the tools. 
\DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
%DIF <   Did we find anything that differs from what the big players have reported (or what was not on their papers at all)
%DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdel{A main threat to the validity of our study relates to external validity and generalization of the findings. These were mitigate by }\DIFdelend \DIFaddbegin \textit{\DIFadd{External validity.}} \DIFadd{concerns generalization of the findings and other threats that can cause incorrect conclusions to be drawn from the study. Despite having a global presence, }\DIFaddend the \DIFdelbegin \DIFdel{inclusion of diverse set of organizations from multiple domains and sizes.
Most of the participating companies were global organizations %DIF < and where possible deviations of the practices in different regions were shared by the practitioners.
While we cannot generalize the findings across the entire population, they do provide insight into the state-of-practice of ML especially in contexts having similar organisational profiles}\DIFdelend \DIFaddbegin \DIFadd{involved organizations are from one geographical location (Finland). This means the conclusions drawn about the state of practice and tools for ML may not be generalized for the whole SE industry population. 
}

\textit{\DIFadd{Reliability.}} \DIFadd{concerns the extent to which data and analysis are dependent on specific researchers. This threat to validity was mitigated by having at least two researchers throughout the research process. Furthermore, the results were shared with practitioners to review before submission for publication}\DIFaddend .


%Huge discrepancies and challenges are observed from the different dimensions of data management practices, particularly those related to data discoverability and accessibility as well as data validation and integrity.



% Monitoring efforts currently focused in data quality, model quality and infrastructure, no observed controls for potential model bias drift or feature attribution (aka explainability)

% Having different size of companies helped identify issues related to smaller organizations. Current literature is based on large organizations with significant resources


