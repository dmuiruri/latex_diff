\subsection{Model training}

\underline{\emph{ML algorithm selection and transfer learning}}
The choice of ML algorithms is influenced by training data type and formulation of the learning problem during \DIFdelbegin \DIFdel{requirement }\DIFdelend \DIFaddbegin \DIFadd{requirements }\DIFaddend elicitation. Heuristics are used in cases H and I to complement ML algorithms; in both cases, an \DIFdelbegin \DIFdel{explicable }\DIFdelend \DIFaddbegin \DIFadd{explainable }\DIFaddend decision based on heuristics is preferred compared to an ML solution with high prediction accuracy but inexplicable. The ML-heuristic trade-off tends to arise due to business sector regulatory constraints.

Transfer learning is typically used to train large NN efficiently, for example, in speech recognition and computer vision settings. This is mainly because model convergence can be a prolonged process that requires significant computing resources. Transfer learning is based on publicly available models or proprietary models.

In cases A and F, computer vision systems utilize transfer learning to test different CNN architectures. Case M's NLP solution is trained using transfer learning to overcome data insufficiency challenges. Case B applies transfer learning based on proprietary models as a cost management strategy. 

Training NN without transfer learning can be driven by two factors observed in cases D and E. One, there is sufficient data and computing resources for training a model to convergence. Two, there is limited availability of suitable open-source models.

\underline{\emph{ML frameworks}}
used across the cases can be broadly categorized as either Neural Network (NN) or classical (non-NN) frameworks. Tensorflow-Keras and PyTorch are the two commonly used frameworks for developing NN models, as summarized in  Table~\ref{tab:data_source_storage_mlframeworks_interviewees}.

Although ML frameworks may provide similar core features, the choice of the framework can be based on a framework's usability, flexibility, or underlying efficiency in utilizing computing resources. For example, both cases D and E develop \DIFdelbegin \DIFdel{ASR }\DIFdelend \DIFaddbegin \DIFadd{automatic speech recognition (ASR) }\DIFaddend models but use Kaldi and PyTorch frameworks, respectively. 
Frameworks can mature into specific domains at varying rates, and therefore teams might adopt different frameworks for such historical reasons. Analytics frameworks such as Spark also feature in case I. 

Overall, challenges in model training relate to infrastructure costs, complexities of tuning, and identifying explainable factors about a model's performance.

% Summary table for practices and challenges
\input{tables/practices_challenges}

\subsection{Model evaluation and experiment management}
Model training is an iterative process with distinct stages; determining the suitability of data and algorithms, parameter and hyper-parameter optimization, and model evaluation. Managing metadata from these stages makes the ML workflow traceable and reproducible.

We note three unique approaches used to evaluate models. One, data is stored according to its quality, which enables composing datasets with different levels of quality for training and validation purposes (Cases D and E). The second approach uses model ensembles, where each model is trained on a unique subset of the data (Case B). The third approach applies a configurable inference algorithm where each configuration uses a unique adaptation of the model (Case E). 

To manage model evaluation results, case organizations either use dedicated experiment tracking tools (case G, I, N, O, and P), log process metadata (case B, E, F) or generate hashes (case D). Hashing involves computing hash values on given combinations of ML artifacts (data, configurations, model) following the execution of an ML pipeline. \DIFdelbegin %DIFDELCMD < 

%DIFDELCMD < %%%
\DIFdelend Case E and F generate and collect metadata (e.g., Git hashes), which are used to produce custom reports. These approaches are summarised in Table~\ref{tab:databases}.

Systematic management of experiments facilitates workflow automation and further increases the traceability and reproducibility of ML workflows.

