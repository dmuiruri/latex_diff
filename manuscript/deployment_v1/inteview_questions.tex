\subsection{Appendix}
\begin{enumerate}
    \item Can you briefly describe in a high-level manner, your model deployment procedure? (how do they get models on the edge/cloud server)
    \item What data processing is done for model inference? (Do you perform any parsing or manipulation of input data during prediction/inference?)
    \item Elaborate in detail the model serving run-time environment, e.g. are you using a custom server, use of containers, use of framework servers (Tensorflow serving)
    \item Describe the prediction/inference service, e.g. how is inference served? Batch/realtime/ REST or gRPC based APIs/in-App Embedded
    \item Describe the model update procedure. i.e., where there is an existing model in production that needs to be upgraded, how is model retraining triggered/done post-deployment?
    \item Can you describe your deployed model monitoring procedures? e.g. what monitoring metrics are used in production?
\end{enumerate}