Case 1: The compiled model binary is packaged as a NodeJS library by binding the model into a nodes module. This allows the model to be imported into other JavaScript applications. Updating the model in this setup begins with generating a new version of the code file and re-compiling the file into a new binary object.

Case 2: The trained models, prepared for deployment, are stored in binary form within an AWS S3 bucket in a designated production folder. The models are manually versioned and typically updated 1-2 times per week, with updates occurring when a newly trained model demonstrates improved offline metrics or when new features are added to the model.

The readiness of a model is assessed through validation using a testing dataset. This involves evaluating several performance aspects of the solution, including the collection of performance metrics such as F1 scores, precision, and recall, as well as testing the real-time inference capability to ensure that predicted events are relevant to the scenes in the video stream.

The binary files of the two models are loaded into a Docker container, also contained in the container is the functions that are used to perform pre-processing and post-processing tasks. Essentially the container contains the entire inference pipeline.

Case 3: Trained models are stored in a cloud server and they are manually copied to the respective inference server together with updated configuration files. A rigorous automated CI/CD pipeline ensures high quality models are deployed, where models undergo testing, building, staging, and production stages. The testing phase involves performing sanity checks and unit tests, followed by building a docker image. An end-to-end integration test is conducted on the resulting image, and the docker image is deployed to a staging environment for re-testing. A version of the model is approved for deployment to the production environment after all tests have passed.

Case 4: The entire process is centered on a CI/CD pipeline (Atlassian Bamboo) that involves first building the model code into a docker container that is used to perform model training, the building process is managed with custom shell or Python scripts. The resulting model artifact moves to a quality assurance stage where it is tested before being serialized and stored in a versioned S3 bucket. Other sanity and basic tests are carried out before the model is deployed, this may involve checking metrics (MAE, RMSE) or resulting data distributions.
The versioned storage allows tracking changes in the model artifact to increase traceability of models. Model are updated routinely based on a schedule (weekly, monthly) or based on business demands; the routine updates are based on re-training workflows configured with Airflow.

Case 5: The ML system consists of 6 models (approx. 350MB each) that provide inference concurrently and each model is trained on a subset of the dataset. The use of multiple models is designed to improve confidence of the overall prediction results through a voting mechanism. One version of each model is stored in an S3 bucket where the model is accessed by other components of the inference architecture. As a retirement policy, once a model is considered for retirement from production, it is stored in an offline server while the online version is deleted from the inference server.

The deployment workflow includes a CI/CD (Bamboo) pipeline with staging and production phases in a cloud environment. The quality assurance process begins with performing local tests on each model by running inference on a dataset of around 1000 documents. These local tests serve as the acceptance tests, and models that pass these tests proceed to the staging environment, where integration and regression testing occurs. Coverage and accuracy metrics are collected at this stage to determine whether the models meet  required quality standards for moving to the production stage. Once in the production environment, an additional testing step is performed using around 20 documents to ensure the model remains functional.

The model binary is packaged into a docker container where it is directly accessed during inference.

Case 6: Models can be added to the platform in two ways: they can either be trained on the platform or uploaded as pre-trained models. However, these models need to be packaged in either PMML (Predictive Model Markup Language) or ONNX (Open Neural Network Exchange) format. During the import process, the platform's parser checks the uploaded model's file for semantic errors and makes corrections where possible. Using open model packaging standards allows the platform to support models trained using different frameworks and the models can be deployed in a variety of computing environments. The platform handles storage for the models, eliminating the need for manual storage management by the developer. Version management is implemented at the project level, the developer manually manages asset level versioning, in this setup, assets belong to a given project. Models can be in one of two states: active or inactive. An active model is kept in memory, while an inactive model is stored in a colder storage location and removed from memory. It is also possible to delete a model completely if desired. If multiple versions of a model are needed, they can be stored as a model group, which allows for the models to be managed together. The system’s workflow does not include quality assurance steps or integrations, such would need to be handled outside of the system.

Case 7: Pre-Deployment: In general, the infrastructure is based on AWS services, EC2 instances for example are used to perform both inference and training tasks, S3 buckets are used for storage of trained models. The pre-deployment workflow operates independently, without being governed by a dedicated CI/CD pipeline framework. The elements are however orchestrated through python scripts and manual interventions. Each customer is associated with a dedicated model and the scale of the operation involves tens of models which adds to the complexity of the pipelines and the general workflows.

Active models are re-trained daily to ensure new data obtained from the campaigns is accounted for by the model. This involves about 100MBs – 1GB of new data depending on the size of the marketing campaign. The CV model is retrained monthly since it is only used to extract features from videos and images. The versioning of the models is based on the S3 storage policy which works both as the model retirement policy.

The quality assurance process is centered around a human-in-the-loop workflow. Before deployment, a human reviewer validates the performance of a new models to ensure that it meets the required metrics, such as F1-score or the accuracy of the CV model used for tagging and labeling images is at the acceptable level.

Re-training of the model is triggered by new incoming data, new labels coming from client for a certain feature or new Ads being pushed by customer previously unseen by the models.

Case 8: The pre-deployment workflow is mainly conducted within the context of DVC pipelines. The pipeline is configured around two workflow stages: i) data preprocessing (parsing to tabular form, scaling) and, ii) model training, tuning, and evaluation, and saving a versioned model. This pipeline can be run locally or in an AWS EC2 instance. The resulting artifacts (model, reports, images etc.) are stored in DVC remote which is connected to an AWS S3 bucket. Versioning of models is implemented through git tags

Quality assurance is based on a CI/CD pipeline which runs after the model has been versioned and committed to DVC for storage.  At this stage, tests are carried out at the code level (unit test) and at the system level which includes testing the DVC pipeline and carrying out end-to-end tests.

The serialized model file is copied to a docker container, and this container among others are run on a designated server (edge box). Transferring the containerized server and other containers to the edge server is done by uploading a zip to an edge server through an edge management system. The zip file contains the necessary runtime libraries, config files, docker-compose files for orchestrating container deployments, the model binary, and the application code. This offline deployment arrangement is required because the edge box is not connected to the internet.