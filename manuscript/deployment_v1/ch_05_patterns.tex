% Discuss pre-deployment as the introduction of this section
This section presents deployment patterns that are derived from the synthesis of deployment practices across the different cases. The deployment patterns are discussed according to the observed activities of the deployment workflow and include (i) model versioning and storage, (ii) quality assurance, (iii) monitoring, (iv) model packaging, and (v) inference serving. 
%Deploying ML models involves a series of tasks that ensure the model will perform optimally in a production environment. We observe that most deployment workflows involve preparatory activities, which we consider pre-deployment activities. These activities often involve model versioning, model storage, quality control and model packaging for inference serving. These activities are accomplished differently across the case companies depending on the maturity level of their machine learning operations (MLOPs), technology stack and problem domain. %In most cases, if other parties use a model, the model is available as an endpoint. However, in some other cases, a team uses the model internally to produce reports and, in this case, may not have an elaborate endpoint serving infrastructure.

\subsection{Model versioning and storage}
Model versioning is the process of keeping track of various iterations of a model artefact during the development workflow. It is an essential aspect of ML development workflow because it increases traceability and reproducibility in a model's life cycle. We note there were two broad ways of achieving model versioning as we observed across the cases: manual and derived versioning of models. By manual versioning, we refer to explicitly assigning versions to a model, while derived versioning refers to the model obtaining its version from a versioned storage container. The most dominant approach was the manual versioning process, where a developer annotates a model's binary file with a number. Such an approach was observed in cases 2, 3, 5, and 6. The other manual versioning process used a dedicated version control tool such as DVC (case 8) and git (case 1). Case 1 used git because the model is stored as a configuration file instead of a binary containing the weights of a trained model. 
%cases 2, 3, 5, and 6.

A derived model versioning approach was observed in cases 3 and 7, where a model was stored in a versioning-enabled S3 bucket. Model objects stored in such a bucket get implicitly versioned, subsequent model uploads are stored, and the underlying versioning feature of the bucket ensures older snapshots are stored and versioned accordingly. The exact behaviour and functionality of the versioning and handling of older versions are configured on the underlying S3 bucket.

Code changes can affect versioning principles in various ways. Case 5 contains multiple components with complex integrations between them. To manage this complexity, the code release cycle is coupled with the model release cycle, which is twice yearly. Although the model and code are independent artefacts, their versioning is connected to a given release. Case 1's model description is contained in a STAN file which, when changed, requires a new version of the model to be generated. Since the model is deployed as an embedded library, changes in the library require a new code release. In case 1, the tight coupling between the model and the code arises due to the model serving pattern.

Data changes affect the versioning principles more so in ML systems where models are updated frequently (e.g. daily) to avoid model staleness. Cases 1 and 7 present scenarios where user behaviours change rapidly, and models must be updated. In such high-change environments, principled versioning of the models may not be as crucial as ensuring new models provide correct predictions. As a solution, a versioning-enabled S3 bucket was applied by case 7. For case 1, the version controlled the model description file, which changed less frequently than the model itself, eliminating the need to version a model. 

The multiplicity of production environments and model customization to users and user groups require an elaborate versioning process. Case 3's deployment is designed for a setup containing two clusters and supports independent customer deployments. Deployed models can share a standard base version, but experience drift at different rates while in production to a point where a new deployment is required. This creates a requirement to maintain multiple versions of a model. To manage this complexity, case 3 uses hashing by using a model's configurations to produce a hash stored as part of the model's metadata. Hash collisions indicate that models are replicas of each other, and different hashes indicate a new model version has been generated. This versioning and version management approach ensures that model integrity and reproducibility can still be achieved in a complex deployment environment.

Serialized models were commonly stored in cloud storage solutions. As new models are installed, we noted that retirement policies were rarely explicitly applied except in case 5. In case 5, the previous versions are deleted from the production server once a new model is deployed. The outgoing model is stored in a local server for future reference.

% \begin{tcolorbox}[colback=blue!5!white,colframe=black!75!black]
%   ML model versions are manually assigned (6/8 cases) or automatically derived (2/8 cases). Manual versioning primarily included annotating a number to a model binary file, whereas in derived versioning, versioned-enabled storage facilitates the model versions. %The model versioning approach is affected by complex integrations in systems with multiple components, high-change (e.g., data) frequency environment and the need for model multiplicity/customization in the production environment. 
% \end{tcolorbox}

%Summary of versioning: version management in ML may require a different approach
 
% version numbering

%Model versioning is essential for several reasons:

%Reproducibility: Model versioning ensures that a specific model version can be reproduced in the future, even if the original code and data are no longer available. This is critical for ensuring that research can be validated and models can be confidently deployed in production.

%Collaboration: Model versioning allows multiple people to work on the same project without interfering with each other's work. Each person can work on a different version of the model, and changes can be merged using version control software.

%Debugging: Model versioning makes it easy to compare different model versions and diagnose errors or performance issues. This is especially important when dealing with complex models or large datasets.

%Compliance: In regulated industries such as healthcare or finance, model versioning is often required to ensure that models meet legal and ethical standards. By keeping track of model versions and their associated metadata, organizations can demonstrate that their models are transparent, fair, and reliable.
%Quality assurance

%Model packaging
\subsection{Quality assurance}
To evaluate quality assurance practices %processes
in the deployment stage of various workflows, we focused on identifying the application of different test cases and automation, for example, through CI/CD pipelines. %testing, validation, monitoring, and logging practices. 
The aim was to establish how teams increase the reliability of the production systems and how they further ensure the models are fit for purpose in the production environment. Applied quality assurance practices ranged from basic quick checks to complex CI/CD pipelines. A combination of factors, such as the complexity of ML workflows, the level of automation in the workflow and the frequency of deployments, influenced 
the quality assurance practices applied across case teams.

Tests for model robustness in the form of basic checks were commonly carried out and involved running basic tests locally using scripts or manual execution of simple tests to validate the model's results, particularly w.r.t accuracy-related metrics. Such results indicate whether a model performs within the scope of acceptable results. We noted that basic sanity tests and manual execution of tests were used as the critical quality gating procedures in setups where models were updated rapidly, typically on daily frequency (case 1, case 7), and multiple models were maintained. Case 7 also involved a human in the loop to validate models manually. Basic checks were also applied (case 4) by checking distributions of prediction results before a model was integrated into the production server. To our observation, there seemed to be a trade-off between introducing added complexity to the workflow, which would arise from automating model evaluation processes and focusing on deploying newly retrained models rapidly to avoid stale predictions. Although the benefits of automated quality management practices were well known to the teams, the effort was focused on ensuring new data was acquired and new models were trained. % No containers is a common characteristic % Challenges of quality assurance in environments of high-frequency deployments

%The overall maturity of MLOps 
The overall level of automation in the ML workflow determines the tooling and automation level applied in managing the quality assurance of the deployment process. We observed that relatively mature setups (cases 3, 4, 5, and 8) had automated testing and validation processes implemented.  CI/CD tools such as Git Actions (case 3) or Bamboo (cases 4 and 5) were used to manage the workflow. The common characteristics among these cases include a low frequency of model updates, complex deployment architectures (cases 3 and 5), containerized components in the architecture, and a part or all of the model inference service used by a third party. A dedicated CI/CD pipeline combines different tests, such as unit tests, integration tests, and regressions, in the deployment process. On the other hand, low level of automation in ML workflow %MLOps maturity environments 
(cases 1, 2 and 7) relied on tests that were run manually but considered sufficient for their workflows. For example, using a human-in-the-loop approach was a factor of low maturity of MLOps processes as opposed to other factors such as regulatory constraints.

 % QA dichotomy between cloud and Edge/IoT deployments
The cases we present are primarily cloud-based systems except for case 6 and case 8, which are IoT solutions. These two cases offer unique perspectives on the quality assurance of ML in IoT settings. Case 6 is a platform designed to provide IoT workflow management. We noted that the platform did not contain built-in software quality management functionalities; somewhat, such functions were anticipated to be managed using other tools external to the platform. The platform is focused on connectivity, data management, analytics and ML. Although models can be developed and deployed iteratively within the platform, functions such as testing and validation of new model versions are conducted in an external environment. The need to support multiple hardware vendors, software stacks, configurations and domain expertise may contribute to this approach, arguably providing platform users more freedom. However, this freedom leads to more complex software delivery workflows in IoT settings.

%Quality assurance is based on a CI/CD pipeline which runs after the model has been versioned and committed to DVC for storage.  At this stage, tests are carried out at the code level (unit test) and the system level, including testing the DVC pipeline and end-to-end tests.
Additionally, case 8 is an IoT solution presented from a specific vendor and an IoT platform user perspective. Further, the IoT platform referenced here differs from the one shown in case 6. We observe that case 8 develops, employs a CI/CD pipeline and packages the models externally to the IoT platform. This lack of an integrated quality assurance process within the IoT platform used was similar to the platform observations made in case 6.

\subsection{Monitoring}
Monitoring in production settings is vital in ensuring deployed models continuously provide acceptable results to users. Deployed models face the risk of drifting, but the negative impact of model drift can vary between use cases. As such, we observed different monitoring practices across the cases w.r.t logging the core application and model performance metrics.
%and taking actions (or issuing alerts) when encountering events that impact model performance.

\textit{Service availability}. Once a model is deployed and available for inference, we note that the high availability of the ML service is one of the most critical aspects across all cases. This level of monitoring ensures all integrations and subsystems are working seamlessly. Teams may use different approaches and tools to achieve this level of monitoring. Cases that provide a prediction service endpoint (cases 1, 2, 3, 4, 5 and 6) to internal teams or third-party users need to ensure the endpoint is accessible and capable of serving predictions. The application level monitoring is done by collecting logs and using systems such as sentry\footnote{https://sentry.io/welcome/}(case 1), AWS API gateway (case 4), Splunk (case 5), cloud watch (case 7), and the IoT platforms also provide visibility to high-level metrics indicating memory footprint (case 6) and health of containers (case 8).
Cases that provide real-time services (case2 and 3) indicated latency as an essential metric monitored to ensure that predictions were produced within acceptable time bounds. Scaling of computing resources is automated using frameworks such as Kubernetes or other serverless deployments in the cloud.  Overall resource monitoring was also a common aspect where tools such as Graphana, Prometheus and logs collection from Kubernetes were commonly applied approaches.

\textit{Performance metrics}. The second commonly monitored aspect is model performance metrics. These metrics are focused on the correctness of the results provided by the model. We categorise the applied approaches into systems that can collect inference data and therefore detect drift because the ground truth is available and systems that may not have ground truth data due to an infeasible labelling process (cases 3 and 5). In cases where the ground truth was unavailable, measures such as edit counts (case 3) and coverage metrics (case 5) were applied to detect drift. % Word Error Rate (WER) could be used in case 3 if ground truth was available. Comment on the category that collects ground truth data.

% packed models are not tested in all cases. This would ensure runtime compatibility and reduces the chances of downtime once a model goes into production.

\subsection{Model packaging and serving
%and serving patterns
}
% Getting the model to the server or edge
% Containerised vs containerized
% Model integration patterns (Model as a service, embedded, etc.)
% Packaging into standardised formats, e.g. ONNX or using platform native binaries

%Model packing involves preparing the ML/DL code and its dependencies from the training phase for deployment in the production environment. The process ensures that the final trained model and its dependencies are optimized for the target environment, and the difference between development and production runtime environments is minimal. In most of our studied cases, a model binary file generated from the compiled ML/DL training code is often saved in specific formats in a storage or directory. For example, Case 6 requires that the models are in PMML and ONNX formats when importing and storing them on the platform. The targeted deployment environment in the cases was mainly a custom/Kubernetes-based server environment.

%\textit{Model Packaging}. 
Packaging of models involves preparing the ML/DL code and its dependencies from the training phase for deployment in the production environment. The process aims to ensure that the final model and its dependencies are optimized for the target environment and that the runtime differences between development and production environments are minimised. We classify model packaging into two broad categories: containerised and uncontainerized. Once packaged, the model serving was done in various ways: model as a service, model as a dependency, precompute or a hybrid of these former ways. 

Most cases (cases 2, 3, 4, 5, and 8) used containerised packaging in their inference architecture. All these cases used docker containers as the containerization technology. Containers provided an execution environment for a stand-alone script in a pipeline and to deploy a model server. A script in this context contained an independent data pre/post-processing step, validation logic or an entire inference pipeline implementation. Containers were used to provide independence of execution steps and the scalability of these steps.

Uncontainerized setups are cases that do not use containers in the model deployment workflow; cases 1 and 7 were such setups. While containers can be beneficial tools in deployment, we observed a tradeoff between gaining the benefits of using a containerised deployment against an increase in deployment overhead, architecture complexity and the added overhead towards the overall maintenance of a deployment pipeline. We associate these observations with settings where models must be rapidly deployed, such as daily deployments (cases 1 and 7). %benefits: scalability, portability, versioning, isolation

The IoT environments presented an additional packaging perspective. Since case 6 is a platform, there is a need to support deploying multiple ML models and frameworks. As a result, the models to be deployed are converted to standardised formats, PMML or ONNX. Supporting these formats adds interoperability and flexibility attributes to the platform and effectively generalizes the platform across multiple kinds of ML use cases. Case 8 packages the code, model, and other data processing stages in docker containers. These container images, all relevant binaries and configurations are compressed into a zip file and uploaded to the target edge device through an IoT platform. The edge device is not connected to the internet, and docker-compose orchestrates the micro-services.

%We note that the model packaging procedure is primarily influenced by the downstream tools and services, including the architectural design choices of the application utilizing the trained model. The following model packaging patterns were observed in the studied cases.

%\subsection{Model serving}
A typical pattern observed in model serving was deploying the model as a service (cases 3, 5, 8). In Case 3, the model was encapsulated in a WebSocket server, and within the microservice architecture, the model is accessed through WebSocket calls. In Case 5, multiple instances are deployed as independent model servers in isolated containers. Similarly, in case 8, the model was served from a separate container within a microservice architecture.

Model serving as a dependency was observed in Cases 2 and 7. In Case 2, the model object was imported as an object in a script. The script also contains the pre and post-processing functionalities of the inference pipeline. In Case 7, the different models used to perform feature engineering and prediction were loaded and accessed in memory from a script. 

Finally, the serving of models as a precompute or a hybrid of the former options was observed in Cases 1 and 4. Case 1 used dependency and precompute patterns where the model is converted to a  Nodes library through binding. This ensures the model can be imported into a NodeJS code base as a library. The model generates predictions stored in a central database from which a web server serves predictions, which forms the architecture's precompute pattern. Case 4 used the model as a dependency pattern to service batch predictions and the model as a service-to-service online prediction.

%\textit{Model binary as a library to embed into an application}. A model binary file is packaged as a library to be imported into other services’ application codes. In Case 1, a model is packaged as a NodeJS module and imported into a JavaScript application. 

%\textit{Model binary as a containerized service to deploy separately from an application}. A model binary file (or object) is accessed directly after exposure as an endpoint via REST/gRPC API. We observed that the model binary file could be packaged into a Docker image (Case 3) or loaded manually to a running image, i.e., a Docker container (Case 2). Furthermore, the containerized service may include pre- and post-processing tasks,  i.e. the entire inference pipeline (Case 2).



\subsection{Inference serving}
% Direct object inference, APIs types (REST/gRPC), inference pipelines, input data processing, batch/online inference
Inference serving is the final stage of deploying a trained model. ML engineers ensure the model can be queried for predictions for a given input data. In this context, we investigated the communication protocols applied, the structure of inference pipelines and the overall inference %model 
serving patterns.

\textit{Online and batch inference}. We categorize online inference into real-time vs non-real-time serving from a latency perspective. The former refers to latency-constrained inference, and the latter relates to latency-tolerant inference. Cases 2 and 3 are video streaming and speech transcription applications, respectively, providing a real-time user experience where inference is constrained to very low latencies for the application to be functional. To support such low latency requirements, we noted Cases 2 and 3 used gRPC and WebSocket protocols in microservice architectures and inference servers. A vital feature of these protocols is the support of bidirectional/streaming communications between a client and server. This feature of these protocols is favourable for services with low latency requirements. The second category of online inference applications is non-real-time applications (Cases 1, 4, 5, and 6). These applications support inference using the HTTP protocol, implemented as REST APIs. Although the models are online, the API response times are not constrained to low latencies or real-time responses.

Batch-based inference refers to scheduled inference on large data sets where the results are often sent into intermediate storage for further processing. Such model use was observed in Cases 1, 4, 5 and 7. In Case 4, an independent model was deployed to support batch inference, and the results were stored in a data lake. Some inference architectures, such as Case 4, are designed to help online and batch inference.
%Other inference architectures do not use any form of endpoints. In this case, the model object is queried directly for inference. 

% It appears that inference architectures and pipelines are designed to optimize specific attributes: latency, resilience, 
\textit{Inference pipelines.} are a structured approach to process requests through steps that ensure a model is queried using correctly formatted data. The initial stage of an inference pipeline is data pre-processing, where the input data is transformed into the form accepted by the model. The second step is the model inference step, where data from the pre-processing stage is used to query inferences from the model. The third step is a post-processing step where inference results are packaged in a format that can be returned to the client. Cases 2, 3, 5 and 8 had implemented inference pipelines and the IoT platform in Case 6 provided a feature to implement inference pipelines. However, these pipelines are realised in various ways across architectures.

Case 2's pipeline is compiled into a single binary object which contains two models and the pre/post-processing functions. The binary object is packaged into a container and later deployed to a Kubernetes environment. This setup simplifies deployment since only a single container is deployed, the entire inference pipeline can be scaled, and the pipeline elements can benefit from reduced latency.

The inference pipeline in Case 3 is a complex setup of pre/post-processing steps containing audio pre-processing, feature extraction, speech recognition, and decoding steps. An acoustic and language model are also deployed in the inference pipeline. The pre-processing, post-processing and models are deployed as independent containers orchestrated using Kubernetes. Decoupling the inference pipeline allows ease of supporting multiple language models since the pre/post-processing steps are not changed. The model servers provide a WebSocket interface which is used to query predictions. % Multi-lingual language models can be implemented in various ways but with varying implications on inference latency, a tradeoff between architectural complexity and latency.

Case 5's inference pipeline is a highly modular architecture involving multiple components to support batch processing of inference requests. The pre-processing step comprises containerised scripts that extract features from the input data and store these features in S3. The inference step of the pipeline contains a controller-responder pattern where the controller issues inference tasks to six responders.  The controller and responders communicate through HTTP requests, and the inference results are stored in JSON format in the S3 bucket. The post-processing step involves two sub-steps of results analysis and de-batching transformations. These sub-steps are implemented as two containerised scripts. Communication across the stages of the inference pipeline is implemented through a messaging system.

Cases 6 and 8 are IoT setups that also use inference pipelines. Since case 6 is an IoT management platform, it supports the creation of inference pipelines where pre/post-processing scripts and the model can be uploaded to the platform. The platform internally generates a pipeline configuration file which can be deployed in the platform as an end-to-end inference solution. Case 8, on the other hand, implements an inference pipeline with pre-processing and post-processing stages deployed as independent containers. The model is also deployed in a separate container. Communication between these containers is supported by messaging using the MQTT messaging protocol. The general use of messaging protocols between microservices was meant to increase the overall resilience of the service.

Model inference may require specialised accelerators such as GPUs, while pre/post-processing stages may not require such specialised hardware. Coupling these functions may lead to the underutilization of GPUs or increased complexity in managing computations between CPUs and GPUs.

% Table: Summary of the patterns
\input{tables/summary_of_patterns}

% ONNX models support more complex inference workflows by having inference pipeline objects which consist of a preprocessing step, inference, and a post-processing step. Pre/post-processing steps are scripts that contain logic to process the data before it gets sent for inference, and the inference results are packaged into JSON format. Such a process is, for example, used in a computer vision solution.

