%\setcounter{subsection}{0}
%\renewcommand*{\theHsection}{chX.\the\value{section}}


\input{tables/tools_summary} % Tooling summary table

%The practices that are used in the operationalization of ML-enabled solutions are supported by a set of tools that cover model training and serving workflows. The training workflow is used to build models often in an offline mode. The trained models are run in production, often in an online mode, through the serving workflow. In this section, we present common tool chains of ML training and serving workflows useful for ensuring repeatable and traceable process of operationalizing ML solutions in industrial settings. 


%\subsection{Provisioning execution infrastructure}


\subsection{Version management}
Model training code often written in notebooks, as well as other project artifacts are version controlled using tools like Git, Gitlab and Bitbucket. Data versioning is done by generating and versioning metadata using specialized tools such as DVC. %Other project artifacts such files or scripts used for documenting: dependencies and versions of open-source libraries (requirements.txt), workflow steps (configuration scripts).
For most cases, model training is conducted in public cloud settings while few cases conduct on-premise training. To consistently provision training environments, 'infrastructure-as-code' practices, using tools like Terraform (Cases A and E). Inference serving is either done in batch or online format.


\subsection{ML training workflow}
We observe that most case organizations containerize (using docker) individual workflow steps as opposed to encapsulating all workflow steps in a single container. %Containerization is appealing because it allows decoupling from the execution environment. 
In ML, containerization facilitates isolation of different workflow tasks/steps which makes the workflow modular, traceable and reproducible. We further note that containers are commonly orchestrated using Kubernetes which allows easier migration of pipelines (or parts of it) across infrastructure vendors. Data transfers across workflow steps during training is done using standard persistent volumes. However, large datasets may require using network mounts (Case F). %, like Amazon Elastic Kubernetes Service (EKS), Google Kubernetes Engine (GKE) and Azure Kubernetes Service (AKS).

ML workflows may include steps specifying feature extraction, model training and validation. The complexity involved in these steps can vary depending on the ML domain. Workflows can be managed using a custom configuration tool (e.g YAML based) or a dedicated workflow toolkit. In complex ML setups, frameworks such as Argo (Case D) and Metaflow (Case G) are preferred. We note that although high-level ML workflow platforms such as AWS SageMaker provide an end-to-end integration advantage, they are also challenging to use when developing complex models due to inflexibility (Cases B, G).

%were considered while in use in some projects within the organisations (Cases B and G), were not used in the studied use cases due to complex requirements of the models, high model training costs and cloud provider dependency. % Note that Case G issues were related to no support for streaming interface and also cloud agnostic. Case A tooling requires adding a YAML file that contains workflow steps in the project repository. The steps vary depending on model complexity and can be visualized using a directional acyclic graph (DAG), which models each step and dependencies between them (Case F).

Those in support of custom tooling appreciate the flexibility to add different tools to the workflow. For example, a tool such as explainer dashboard which is used to facilitate a model's explainability is added as part of a ML workflow (Case A). An alternative workflow setting can have a single step containing multiple containers (data access data and model training). Customized components that provide access to these containers can be created to monitor independent utilization of computing resources at the container level (Cases C, G).

One overall advantage of using ML workflow tools is that event based training queues can be orchestrated for example based on continuous arrival of training data.  Tools like Apache airflow provide the functionality to schedule model training based on given triggers. %In Case 8, a consumer API is used to provide a common interface for accessing containers, actual data and the different queues.  

%dedicated workflow frameworks are preferred as opposed to custom solutions. When a single task contains multiple containers, custom solutions involves implementing components or agents that provide an interface to a container for accessing data and compute resources during training (Cases C, G). In addition, other components in the custom ML workflow solution, such as explainer dashboard (https://bit.ly/3Byj1js) (Case A) to facilitate a model's explainability if required.

%and pipeline orchestrator. The pipeline orchestrator of Case C ensures the transition of tasks between queues since each container has its queue

%The ML training workflow frameworks are used in some cases rather than a custom solution to provide automated end-to-end integration of workflow steps. . 
% solutions 
 
ML experiments can be traced using custom web-based UI tools, this facilitate evaluation of results and model performance comparison during the development process (Cases B and F). To their advantage, custom platforms can freely include any metadata the team considers relevant (Case F). Plugins can also be developed to integrate with existing open-source solutions such as MLflow (Case G). Low-level training metrics are observed with Tensorboard (Case A).

\subsection{Continuous integration and testing}

Continuous integration tools such as Jenkins are used to run tests and build docker images based on model artifacts resulting from the training workflow (Cases A, D, G). Static code analysis and other tests are used to check general container functionality during the image building process (Case A, G). Domain-specific tests are also executed to ensure the scope of a model's inputs and outputs is unchanged. These types of tests generally extend testing to the entire pipeline using small amounts of input data (Case D, F, G). Docker images created from the CI system are (automatically) deployed to a staging environment for additional tests before to deployment. %The docker images are of various sizes, for example up to 12GB in Case D. 
%For Case C, the latter environment contains a copy of production data which due to restrictions was not accessible in other environments. 
Typically this may include testing the model API's data type (Case A, D), ensuring a model makes sound predictions (Case E) and also ensuring that the deployment procedure loads a serialized model into the relevant API endpoints.

Trained models are generally stored in classic data storage solutions or dedicated container image registries. For example, Case E stores a model and metadata about the model to a PostgreSQL data warehouse. On the other hand, case C stores trained models in Nexus while case G uses docker registry to store models prior to their production deployment.

% Summary table for practices and challenges
\input{tables/practices_challenges}

\subsection{ML deployment and serving}
Majority of the models are deployed as REST (Representational State Transfer) API endpoints on public cloud or on-premise servers. Other deployment targets include embedding the model on the actual application such as a mobile application (Case P) or deploying to IoT devices through over-the-air deployments or onsite installations.

Models with strict inference latency requirements are deployed as gRPC (Google Remote Procedure Call) API endopoints. Case C business application has strict latency requirements and therefore makes use of gRPC which supports streaming. %For ML solutions that need to serve inference in (or near) real-time, different strategies are continuously evaluated both at models level and services level (Case E, G). 

Most cases implement custom serving infrastructure although emerging model serving systems like KFServing and Seldon are being tested in Cases C and O respectively. 

Finally, we note that deployment related tasks are often not undertaken by data scientists but by other dedicated team members with specialist knowledge for example knowledge on Kubernetes configuration (Case G).

%While data scientists build up until model container images are created for deployment, the actual process of deploying the containers to production is more involving requiring expertise and deep understanding of the tools used in the serving infrastructure e.g., Kubernetes, amongst other things. The platform teams are looking to automate the deployment process (Case G).


% Case D, models are deployed on inference servers, with separate clusters for client with high-traffic volume. A main server cluster runs about 20 different models at the same time and each model is several gigabytes of memory. The inference infrastructure is setup in a way that " but different versions of the server have different models, and the routing is automatically happening."?? Servers are made to automatically scale with increase in traffic when new models are deployed. Monitoring is done with Prometheus and grafana for dashboards. Other than ensuring tha the model is running, monitoring of model accuracy in production is not feasible especially when data is owned externally by the client (Case D). Due to strict separation of the data, the inference system serves the prediction instantly and no data is stored.

% Case G ...And then so now, so the data scientists can like themselves go all the way to our supposedly working container. But then when you want to deploy that container, so actually live endpoint to do inference on them. At this point, we, you need some Kubernetes knowledge. I mean, in principle, the data scientists can do that themselves, but it is a more involved process. So usually, someone from the platform team helps them at this point. But this is an area of active investigation, how to make it more self serve. So that data scientists could go in like this sort of DevOps fashion, all the way from development to production.

\subsection{Monitoring}
After model deployment, monitoring is performed at different levels of granularity. The most common form of monitoring is undertaken for infrastructure management. Where logging, monitoring and alerting services and tools like Prometheuse and Kubernetes logging (stackdriver) are used to collect a cluster's performance metrics which are visualised on dashboards using tools like Grafana, Tableau or any other business analytics tools. For models deployed as API, model logging (e.g., model predictions) integration with services like Elasticsearch and BigQuery can be used to perform model health and quality checks in production e.g., average accuracy on sampled log (Cases A, F and O).


% case G  would say that monitoring is something we are very well aware that is sorely needed. But that which is still like an open question, how to solve it pretty much are you like, including what sort of operational teams to to be involved kind of maintaining the system ..And, yeah kind of we haven't yet gone into like the SLA s and stuff like that, because it's the lot in production. So we have been able to postpone it. But there are different while there's monitoring, and so many different levels, so, for example, you would want to know, like even when you're training or, or like you, if there's something going wrong, you need to be able to pinpoint at which point on which machine, something went wrong, and probably you won't be able to replicate the whole training process with the same data set this whole replication issue. And I feel like, we are probably not very, in a very good position with that, because the data platforms are not that we are using are not like we dislike data. versioning stuff, I think, is something we will probably run into later. But it is not solved yet.So this true monitoring on the training level, we have some tracing stuff. So we like on the network level, we know how the requests go. But that is like a super low level stuff. And if you there, we don't have like this, we would want to have is like the single pane of glass where we could see that this model is not being trained and it's moving, MLflow tries to be that a little bit, but it doesn't give like an active view into many parts of the system.And then on the endpoint level, of course, like when you have actually endpoints running and I mean, we will need endpoint level monitoring, where there are health checks and all kinds of quality metrics flowing in all the time, but that's not in place.

% Summarise RQ2, the pipeline section
Maintaining the collective set tools used across a pipeline can develop into a complex task especially when dealing with NN architectures.

Generally, we observe that practices vary across organizations based on factors such as the type of data being used for training, availability of computing resources and type of ML solution being developed. We also note there are two primary ways case organizations develop their pipelines, (1) compose a variety of tools to orchestrate the pipeline or (2) apply a more encompassing framework such as Sagemaker which contains inbuilt tools for different parts of a pipeline. We note that most teams prefer flexibility and the ability to extract low level information provided by independent tools, i.e. the first approach. While the few teams that use the second approach prefer the instant integration of tools provided by their cloud provider. We summarize all general practices in Table~\ref{tab:practices_challenges}.
