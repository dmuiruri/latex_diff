% Monitoring for Model quality using accuracy metrics, monitoring data drift using things like histograms, descriptive statistics .  
\subsection{Model monitoring} 

\underline{\emph{Training data drift}} commonly occurs due to structural changes in the data generating process. Identifying drift in numeric data types is be achieved by using visual tools such as graphs or descriptive statistics (cases G, H, I), image-based data makes use of histograms (case F). Speech and text-based data is also susceptible to drift but can be more challenging to monitor. For example, case D mentioned the emergence of the word Covid-19 in the medical sphere recently, but the word is not available in any historical corpus. Typically, heuristics are used to monitor drift in these speech or NLP settings.

\underline{\emph{Model drift}} can occur as a result of data or concept drift and it is often manifested by a loss in a model's accuracy. Observing metrics such as accuracy and error rates are the common ways production models are monitored. For example, in a transcription setting, measuring the character and word edits required after inference were used (Case D) as error metrics to monitor production models and to characterise any drift in the model.

\underline{\emph{Infrastructure monitoring}}
is applied to ensure models utilize resources (GPU/CPU, memory, disk, network, etc.) efficiently or to flag technical problems such as scaling designs and I/O bottlenecks during training or inference. Cases D, E and G closely monitor endpoint latency since it forms an important requirement of the entire ML solution.

%Case organizations are aware of the benefits of monitoring different parts  of the entire ML pipelines but often times end up monitoring only certain stages of the pipelines based on their perceived importance.

