% Table interview questions

\DIFdelbegin %DIFDELCMD < \begin{table}[ht]
%DIFDELCMD <     %%%
\DIFdelendFL \DIFaddbeginFL \begin{table}
    \DIFaddendFL %\fontsize{9pt}{8pt}\selectfont
    \caption{Interview Questions}
    \DIFdelbeginFL %DIFDELCMD < \resizebox{\columnwidth}{!}{
%DIFDELCMD <     \begin{tabular}{p{0.1cm}p{7cm}} 
%DIFDELCMD <     \hline
%DIFDELCMD <         1 & Can you briefly describe, in a high-level manner, your model deployment procedure? (how do they get models on the edge/cloud server) \\   
%DIFDELCMD <         

%DIFDELCMD <         2 & What data processing is done for model inference? (Do you perform any parsing or manipulating input data during prediction/inference?) \\
%DIFDELCMD <         

%DIFDELCMD <         3 & Elaborate in detail the model serving run-time environment, e.g. are you using a custom server, use of containers, use of framework servers (Tensorflow serving) \\
%DIFDELCMD <         

%DIFDELCMD <         4 & Describe the prediction/inference service, e.g. how is inference served? Batch/realtime/ REST or gRPC-based APIs/in-App Embedded \\
%DIFDELCMD <         

%DIFDELCMD <         5 & Describe the model update procedure. i.e., where an existing model in production needs to be upgraded, how is model retraining triggered/done post-deployment? \\
%DIFDELCMD <         

%DIFDELCMD <         6 & Can you describe your deployed model monitoring procedures? e.g. what monitoring metrics are used in production? \\
%DIFDELCMD < 

%DIFDELCMD <     \hline    
%DIFDELCMD <     \end{tabular}
%DIFDELCMD <     }
%DIFDELCMD <   %%%
\DIFdelendFL \DIFaddbeginFL \begin{tabular}{p{0.1cm}p{13cm}} 
    \hline
        \DIFaddFL{1 }& \DIFaddFL{Can you briefly describe, in a high-level manner, your model deployment procedure? (how do they get models on the edge/cloud server) }\\   

        \DIFaddFL{2 }& \DIFaddFL{What data processing is done for model inference? (Do you perform any parsing or manipulating input data during prediction/inference?) }\\

        \DIFaddFL{3 }& \DIFaddFL{Elaborate in detail the model serving run-time environment, e.g. are you using a custom server, use of containers, use of framework servers (Tensorflow serving) }\\

        \DIFaddFL{4 }& \DIFaddFL{Describe the prediction/inference service, e.g. how is inference served? Batch/realtime/ REST or gRPC-based APIs/in-App Embedded }\\

        \DIFaddFL{5 }& \DIFaddFL{Describe the model update procedure. i.e., where an existing model in production needs to be upgraded, how is model retraining triggered/done post-deployment? }\\

        \DIFaddFL{6 }& \DIFaddFL{Can you describe your deployed model monitoring procedures? e.g. what monitoring metrics are used in production? }\\

    \hline    
    \end{tabular}
  \DIFaddendFL \label{tab: interview_questions}
\end{table}

