\subsection{Model training}

\underline{\emph{ML algorithm selection and transfer learning}}
are commonly occurring practices. Selection of ML algorithms is largely influenced by training data type and formulation of the learning problem during requirement elicitation. Heuristics are used in cases H and I to complement ML algorithms, in both cases, an explicable decision based on heuristic algorithms is highly regarded compared to an ML solution with high accuracy but largely inexplicable. The trade-offs arise either due to regulatory constraints or where a heuristic based approach provides a much simpler solution compared to a complex ML with a closely similar result.

%A similar learning problem can be approached in different ways, for example the problem of extracting information from documents addressed in Cases B and C. A pre-study conducted in Case B showed an NLP approach not to be fit for their problem instead two supervised ML models are used. One model is based on convolutional variational layers using a Bayesian approach and mostly handles non-machine readable documents with an image of the document as input. The second model is a convolutional pyramid model based on convolutional neural networks (CNN) layers used mostly for machine readable invoices with a chargrid compressed image as input. A clustering algorithm is also used in the second model to automatically group the documents into similarity groups e.g., documents from different suppliers. Case C implements classifiers based on CNN to recognize the types of document images.

Transfer learning is indicated as the main approach to train NN efficiently since model parameters can take a long time to converge and require significant computing resources. Transfer learning is based either on publicly available models or proprietary models. 

Computer vision systems in cases A and F make use of transfer learning by applying state of the art models available on a wide range of CNN architectures. Case M's NLP solution was also trained using transfer learning mainly to overcome data insufficiency challenges. Case B applies transfer learning based on proprietary models as a cost management strategy. 
%The setup consists of an ensemble of six models and each model is trained for 15 epochs. If the training data set is too large, one epoch can take up to 5 hours, meaning a total of 450 hours computing hours to train the entire ensemble. When transfer learning is applied, only the last 4 epochs are trained which provides significant cost reduction. Essentially, case B maintains two training cycles, a low frequency training of large models and higher frequency training cycle based on transfer learning.

Training NN without using transfer learning can also be motivated by several factors we observe in case D and E. (1) The amount of data is considered sufficient for training a model to convergence. (2) Availability of required computing resources. (3) Limited availability of relevant open-source models in a domain. %Case D and E projects train the ASR models with their own data. Research grants or credits provided by their cloud providers facilitated access to computing infrastructure in the early phase of the project. A full model training cycle in case E can relatively take two weeks, for example Case D's training data had low resource for Finnish languages within the healthcare sector.

\underline{\emph{ML frameworks}}
used across the cases can be broadly categorized as either Neural Network (NN) or classical (non-NN) ML solutions. Tensorflow (https://bit.ly/38sgWc4) and PyTorch (https://bit.ly/3gMHnxG) are the two commonly used frameworks for developing DL models as summarised in Table~\ref{tab:data_source_storage_mlframeworks}. Practitioners who used TensorFlow tended to make use of the Keras (https://keras.io/) framework which abstracts the low-level syntax found in the native TensorFlow framework.

Although NN frameworks provide similar core features, a few factors can affect the choice of framework. (1) a framework's usability, (2) a framework's underlying efficiency in utilizing computing resources, (3) a framework's flexibility. A case in point, cases D and E develop an ASR solutions but make use of Kaldi and PyTorch frameworks respectively. %Similarly, NLP models in cases M and N were developed using PyTorch and Watson (https://ibm.co/3BrGa6P) respectively. 
Frameworks can mature into certain domains much later and therefore teams might seemingly use different frameworks out of such historical reasons. %In non-DL setups, Scikit-Learn (https://bit.ly/3t2jyqH) and XGBoost (https://bit.ly/3t4lKhw) were dominant frameworks/libraries used to implement ML solutions.
Specialised analytics frameworks such as Spark (https://bit.ly/3gLVtQ7) also feature in case I. We generally note that team members freely adopt frameworks suitable for accomplishing tasks efficiently. %This was however best supported in two scenarios: (1) Teams with a pre-defined data type abstraction as discussed in the previous data storage formats section, accompanied by a dedicated team to manage deployment operations e.g., in case G. (2) When modelling results do not require elaborate deployment techniques, often the end result is a report or exploratory analysis. 
%For instance, Case N indicated one team member's preference to use R (https://bit.ly/3zwVoqK) and RStudio (https://bit.ly/38s1hcS) in generating reports and analysis while the rest of the team mainly uses Python and Python based frameworks.
% Ad-hoc experiments
% An alternative view pertaining to model training relates to whether the setup is an IoT or a non-IoT setting. IoT settings often mean that data is streamed from multiple devices or sensors and setups in cases A, I, and J were observed to l

%The ML problem of extracting information from documents, is addressed in Cases B and C using different approaches. A pre-study conducted in Case B showed an NLP approach not to be fit for their problem instead two supervised ML models are used. One model is based on variational convolutional of layers using a Bayesian approach and mostly handles non-machine readable invoices. The model takes as input an image of an invoice and outputs the extracted fields from the image. Specifically, an OCR is used to extract characters from the image and these are combined and fed into the model so as to get the recognized values. The second model is a convolutional pyramid model based on CNN layers used mostly for machine readable invoices. The second model takes as input a chargrid compressed image of an invoice and outputs a heatmap that highlights areas of interest for text extraction. A clustering algorithm is also used in the second model to automatically group the invoices into similarity groups e.g., invoices from different suppliers. On the other hands, Case C implements classifiers based on CNN to recognize the types of document images 
% Case C: for the fieldsâ€™ recognition side, so we basically try to identify which document arrives to us. And we were testing convolutional neural networks, and see how these convolutional neural networks, and text analysis, and combination of these, the accuracy is something like fully around 93/ 94%. And once we know what document we have at hand, then we need to see the kind of template to, because we are getting a lot of pictures taken with a smartphone and they have skew errors,  perspective errors and we're trying to fix them with keypoint descriptors, with image alignment, basically. And once we know that, and we know in which x and y coordinates we have relevant fields in the templates that we can OCR the specific coordinates. A bit unstructured data a bit more structured
% For Case B, models are trained using Tensorflow and one training cycle is for six models trained from the scratch. For re-training transfer learning is used to train pre-trained models on new data.

Overall, challenges in model training relate to infrastructure costs, complexities of tuning and identifying explainable factors about a model's performance. 

\subsection{Model evaluation and experiment management}
Model training is considerably an iterative process involving (1) determining suitability of data and algorithms, (2) parameter and hyperparameter optimization and (3) model evaluation. 
%These iterations result in multiple model variants with their respective attributes, such as accuracy and parameter settings, which calls for 
Managing metadata from these experiments makes the training process traceable and reproducible.

We note three unique approaches used to evaluate models: (1) Data is stored such that it can be stratified by quality allowing composition of training and validation data to include different quality (Cases D and E), (2) Use of ensemble of models each trained on a unique subset of the data (Case B) and (3) Use of a configurable inference algorithm where each configuration makes use of a unique adaptation of the model (Case E). %In the third setup, a model is composed of a core part supporting feature sharing and different adaption layers tuned for specific use cases. Each configuration of the model has an associated test data used to validate the model. This was encountered in case E where 30 to 40 configurations of the model were maintained.

To manage model evaluation results from these kinds of setups, case organizations either use dedicated experiment tracking tools case (G, I, N, O and P), logging process metadata (case B, E, F) or generating hashes (case D). These approaches are summarised in Table~\ref{tab:databases}. 

Case D uses hashing such that a hash is computed from a given version of the data combined with a model's parameters. The resulting hash is stored for later reference. Obtaining a previously stored hash implies that a model if similar characteristics already exists. %based on that data version and parameter settings has been previously trained and the model can be identified by a given version number. 

Case E and F utilize the generation and collection of metadata which includes metadata collected from tools such as git hashes. Case E stores metadata in a data warehouse which can be queried to produce spreadsheets reports. Case F's platform generates metadata at each step of the pipeline and resulting data is visualized on a web tool.
Systematic management of experiments facilitates workflow automation. %The same metadata can be utilised by the MLOps platform to re-run a model using a previous configuration.

%Case P tracks experiments using two approaches. One approach makes use of the MLflow (https://bit.ly/38sttfK) tool. The second approach allows data scientists to freely generate metrics in JSON format and store them in DynamoDB (https://amzn.to/3mMHGwh). The second approach facilitates storage of metrics without following any strict schema being enforced because of the NoSQL nature of the DynamoDB. 

%In the overall, we observed that such systematic management of experiments facilitates workflow automation since data required to repeat a tracked stage of the pipeline is readily available and the transparency of the entire pipeline increased. 
