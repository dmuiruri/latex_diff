% Based on what is covered in section ML serving workflow, this section would be redundant.

% Models are deployed to support batch or online inference. In online inferencing, some models are required to support real-time inferencing with strict latency times.
